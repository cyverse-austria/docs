{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home \u00b6 Welcome to the CyVerse Austria! CyVerse provides life scientists with powerful computational infrastructure to handle huge datasets and complex analyses, thus enabling data-driven discovery. Our extensible platforms provide data storage, bioinformatics tools, image analyses, APIs, and more. CyVerse Austria uses open source code from CyVerse US (University of Arizona) and its deployment is funded by BioTechMed-Graz . Within the program Digitale TU Graz the follow-up project for expansion of CyVerse Austria (Austrian DataLab and Services) is funded by the BMBWF What is Cyberinfrastructure? \u00b6 Cyberinfrastructure (also known as CI or computational infrastructure) provides solutions to the challenges of large-scale computational science. Just as physical infrastructure such as laboratories makes it possible to collect data, the hardware, software, and people that comprise cyberinfrastructure make it possible to store, share, and analyse data. Using cyberinfrastructure, teams of researchers can attempt to answer questions that previously were unapproachable because the computational requirements were too large or too complex. CyVerse cyberinfrastructure includes: A data storage facility An interactive, web-based, analytical platform Web authentication and security services Support for scaling computational algorithms to run on large, high-speed computers Publication of data by issuing persistent identifiers (e.g. DOI) Education and training in how to use cyberinfrastructure History \u00b6 CyVerse was originally created by the U.S. National Science Foundation in 2008 with the name iPlant Collaborative. From its inception, iPlant quickly grew into a mature organization providing powerful resources and offering scientific and technical support services to researchers nationally and internationally. In 2015, iPlant was rebranded to CyVerse to emphasize an expanded to serve all life sciences. Development \u00b6 coming soon > Getting Started Services \u00b6 coming soon > Services Guide","title":"Home"},{"location":"#home","text":"Welcome to the CyVerse Austria! CyVerse provides life scientists with powerful computational infrastructure to handle huge datasets and complex analyses, thus enabling data-driven discovery. Our extensible platforms provide data storage, bioinformatics tools, image analyses, APIs, and more. CyVerse Austria uses open source code from CyVerse US (University of Arizona) and its deployment is funded by BioTechMed-Graz . Within the program Digitale TU Graz the follow-up project for expansion of CyVerse Austria (Austrian DataLab and Services) is funded by the BMBWF","title":"Home"},{"location":"#what-is-cyberinfrastructure","text":"Cyberinfrastructure (also known as CI or computational infrastructure) provides solutions to the challenges of large-scale computational science. Just as physical infrastructure such as laboratories makes it possible to collect data, the hardware, software, and people that comprise cyberinfrastructure make it possible to store, share, and analyse data. Using cyberinfrastructure, teams of researchers can attempt to answer questions that previously were unapproachable because the computational requirements were too large or too complex. CyVerse cyberinfrastructure includes: A data storage facility An interactive, web-based, analytical platform Web authentication and security services Support for scaling computational algorithms to run on large, high-speed computers Publication of data by issuing persistent identifiers (e.g. DOI) Education and training in how to use cyberinfrastructure","title":"What is Cyberinfrastructure?"},{"location":"#history","text":"CyVerse was originally created by the U.S. National Science Foundation in 2008 with the name iPlant Collaborative. From its inception, iPlant quickly grew into a mature organization providing powerful resources and offering scientific and technical support services to researchers nationally and internationally. In 2015, iPlant was rebranded to CyVerse to emphasize an expanded to serve all life sciences.","title":"History"},{"location":"#development","text":"coming soon > Getting Started","title":"Development"},{"location":"#services","text":"coming soon > Services Guide","title":"Services"},{"location":"database/de-db/","text":"DE Database \u00b6 This Database is used for CyVerse discovery environment. Initialize database \u00b6 Access Database \u00b6 ssh root@DB_HOST.com psql -U postgres Create required Database and User \u00b6 # create de user create user de with password '********' ; # create de database create database de with owner de ; Add required extensions \u00b6 # psql -U postgres \\c de create extension \"uuid-ossp\" ; create extension \"moddatetime\" ; create extension \"btree_gist\" ; Populate / Migrate the Database \u00b6 Ensure the following before running the command: - Docker is installed on your host machine. - The host has network access to the target PostgreSQL database. Clone the migration repository: git clone https://github.com/cyverse-austria/de-database.git Run the migrations using Docker: docker run --rm \\ -v $( pwd ) /de-database/migrations:/migrations \\ --network host \\ migrate/migrate \\ --database \"postgres://de: $DE_PASSWORD @ $DB_HOST /de?sslmode=disable\" \\ -path /migrations \\ up Additional database queries \u00b6 # checkout to de user psql -U de SET search_path = public, pg_catalog ; # create table version CREATE TABLE version ( version character varying ( 20 ) NOT NULL, applied timestamp DEFAULT now () ) ; # populate the Version table from a file # location of 999_version.sql: # https://github.com/cyverse-de/de-database/edit/master/old-databases/de-db/src/main/data/999_version.sql psql -U postgres -h localhost -d de -f 999_version.sql","title":"DE Database"},{"location":"database/de-db/#de-database","text":"This Database is used for CyVerse discovery environment.","title":"DE Database"},{"location":"database/de-db/#initialize-database","text":"","title":"Initialize database"},{"location":"database/de-db/#access-database","text":"ssh root@DB_HOST.com psql -U postgres","title":"Access Database"},{"location":"database/de-db/#create-required-database-and-user","text":"# create de user create user de with password '********' ; # create de database create database de with owner de ;","title":"Create required Database and User"},{"location":"database/de-db/#add-required-extensions","text":"# psql -U postgres \\c de create extension \"uuid-ossp\" ; create extension \"moddatetime\" ; create extension \"btree_gist\" ;","title":"Add required extensions"},{"location":"database/de-db/#populate-migrate-the-database","text":"Ensure the following before running the command: - Docker is installed on your host machine. - The host has network access to the target PostgreSQL database. Clone the migration repository: git clone https://github.com/cyverse-austria/de-database.git Run the migrations using Docker: docker run --rm \\ -v $( pwd ) /de-database/migrations:/migrations \\ --network host \\ migrate/migrate \\ --database \"postgres://de: $DE_PASSWORD @ $DB_HOST /de?sslmode=disable\" \\ -path /migrations \\ up","title":"Populate / Migrate the Database"},{"location":"database/de-db/#additional-database-queries","text":"# checkout to de user psql -U de SET search_path = public, pg_catalog ; # create table version CREATE TABLE version ( version character varying ( 20 ) NOT NULL, applied timestamp DEFAULT now () ) ; # populate the Version table from a file # location of 999_version.sql: # https://github.com/cyverse-de/de-database/edit/master/old-databases/de-db/src/main/data/999_version.sql psql -U postgres -h localhost -d de -f 999_version.sql","title":"Additional database queries"},{"location":"database/de-releases/","text":"Metadata Database \u00b6 This Database is used for CyVerse De Releases. Initialize database \u00b6 Access Database \u00b6 ssh root@DB_HOST.com psql -U postgres Create required Database \u00b6 # create metadata database with de user create database de_releases with owner de ; Add required extensions \u00b6 # psql -U postgres \\c de_releases create extension \"uuid-ossp\" ; create extension \"moddatetime\" ; create extension \"btree_gist\" ; Populate / Migrate the Database \u00b6 Ensure the following before running the command: - Docker is installed on your host machine. - The host has network access to the target PostgreSQL database. Clone the migration repository: git clone https://github.com/cyverse-austria/mgmt.git Run the migrations using Docker: docker run --rm \\ -v $( pwd ) /mgmt/db/migrations:/migrations \\ --network host \\ migrate/migrate \\ --database \"postgres://de: $DE_PASSWORD @ $DB_HOST /de_releases?sslmode=disable\" \\ -path /migrations \\ up Note: Replace $DE_USER, $DE_PASSWORD, and $DB_HOST with the appropriate environment variables or values.","title":"De Releases"},{"location":"database/de-releases/#metadata-database","text":"This Database is used for CyVerse De Releases.","title":"Metadata Database"},{"location":"database/de-releases/#initialize-database","text":"","title":"Initialize database"},{"location":"database/de-releases/#access-database","text":"ssh root@DB_HOST.com psql -U postgres","title":"Access Database"},{"location":"database/de-releases/#create-required-database","text":"# create metadata database with de user create database de_releases with owner de ;","title":"Create required Database"},{"location":"database/de-releases/#add-required-extensions","text":"# psql -U postgres \\c de_releases create extension \"uuid-ossp\" ; create extension \"moddatetime\" ; create extension \"btree_gist\" ;","title":"Add required extensions"},{"location":"database/de-releases/#populate-migrate-the-database","text":"Ensure the following before running the command: - Docker is installed on your host machine. - The host has network access to the target PostgreSQL database. Clone the migration repository: git clone https://github.com/cyverse-austria/mgmt.git Run the migrations using Docker: docker run --rm \\ -v $( pwd ) /mgmt/db/migrations:/migrations \\ --network host \\ migrate/migrate \\ --database \"postgres://de: $DE_PASSWORD @ $DB_HOST /de_releases?sslmode=disable\" \\ -path /migrations \\ up Note: Replace $DE_USER, $DE_PASSWORD, and $DB_HOST with the appropriate environment variables or values.","title":"Populate / Migrate the Database"},{"location":"database/grouper-db/","text":"Grouper Database \u00b6 This Database is used for CyVerse Grouper service. Initialize database \u00b6 Access Database \u00b6 ssh root@DB_HOST.com psql -U postgres Create required Database and User \u00b6 ## create grouper user create user grouper with password '********' ; ## create grouper database create database grouper with owner grouper ; Add required extensions \u00b6 # psql -U postgres \\c grouper create extension \"uuid-ossp\" ; create extension \"moddatetime\" ; create extension \"btree_gist\" ; Note: The Ansible playbooks handle database initialization and population automatically.","title":"Grouper Database"},{"location":"database/grouper-db/#grouper-database","text":"This Database is used for CyVerse Grouper service.","title":"Grouper Database"},{"location":"database/grouper-db/#initialize-database","text":"","title":"Initialize database"},{"location":"database/grouper-db/#access-database","text":"ssh root@DB_HOST.com psql -U postgres","title":"Access Database"},{"location":"database/grouper-db/#create-required-database-and-user","text":"## create grouper user create user grouper with password '********' ; ## create grouper database create database grouper with owner grouper ;","title":"Create required Database and User"},{"location":"database/grouper-db/#add-required-extensions","text":"# psql -U postgres \\c grouper create extension \"uuid-ossp\" ; create extension \"moddatetime\" ; create extension \"btree_gist\" ; Note: The Ansible playbooks handle database initialization and population automatically.","title":"Add required extensions"},{"location":"database/irods-db/","text":"iRODS Database \u00b6 This Database is used for iRODS. current postgresql version: 12 . Create required Database and User \u00b6 After installing a postgresql database we would need to create required users and Database . --- -- (ICAT) create database CREATE DATABASE \"ICAT\" ; -- (icat_reader) This user will access the database from discovery environment CREATE USER icat_reader WITH ENCRYPTED PASSWORD 'YOUR_PASSWORD' ; GRANT ALL PRIVILEGES ON DATABASE \"ICAT\" to icat_reader ; -- (irods) This user will access the database from the iRODS server CREATE USER irods WITH ENCRYPTED PASSWORD 'YOUR_PASSWORD' ; GRANT ALL PRIVILEGES ON DATABASE \"ICAT\" to irods ; Initialize Database \u00b6 Database initialization is fully automated and managed through Ansible playbooks, specifically handled by the setup_irods.yml task. Grant icat_reader Database permissions \u00b6 Run this only after the initialize of iRODS database . --- \\c ICAT GRANT SELECT , INSERT , UPDATE , DELETE ON ALL TABLES IN SCHEMA \"public\" TO icat_reader ; Post-Deployment Steps \u00b6 Also see the post-iRODS deployment documentation for additional configuration steps.","title":"iRODS Database"},{"location":"database/irods-db/#irods-database","text":"This Database is used for iRODS. current postgresql version: 12 .","title":"iRODS Database"},{"location":"database/irods-db/#create-required-database-and-user","text":"After installing a postgresql database we would need to create required users and Database . --- -- (ICAT) create database CREATE DATABASE \"ICAT\" ; -- (icat_reader) This user will access the database from discovery environment CREATE USER icat_reader WITH ENCRYPTED PASSWORD 'YOUR_PASSWORD' ; GRANT ALL PRIVILEGES ON DATABASE \"ICAT\" to icat_reader ; -- (irods) This user will access the database from the iRODS server CREATE USER irods WITH ENCRYPTED PASSWORD 'YOUR_PASSWORD' ; GRANT ALL PRIVILEGES ON DATABASE \"ICAT\" to irods ;","title":"Create required Database and User"},{"location":"database/irods-db/#initialize-database","text":"Database initialization is fully automated and managed through Ansible playbooks, specifically handled by the setup_irods.yml task.","title":"Initialize Database"},{"location":"database/irods-db/#grant-icat_reader-database-permissions","text":"Run this only after the initialize of iRODS database . --- \\c ICAT GRANT SELECT , INSERT , UPDATE , DELETE ON ALL TABLES IN SCHEMA \"public\" TO icat_reader ;","title":"Grant icat_reader Database permissions"},{"location":"database/irods-db/#post-deployment-steps","text":"Also see the post-iRODS deployment documentation for additional configuration steps.","title":"Post-Deployment Steps"},{"location":"database/keycloak-db/","text":"Keycloak Database \u00b6 This Database is used for Keycloak. Initialize database \u00b6 Access Database \u00b6 ssh root@DB_HOST.com psql -U postgres Create required Database and User \u00b6 # create keycloak user create user keycloak with password '********' ; # create keycloak database create database keycloak with owner keycloak ; Note: The database initializes automatically. Backup Database \u00b6 To backup the database we will use the pg_dump , make sure this plugin is installed. # -U user -d database pg_dump -U keycloak -d keycloak > keycloak.sql Issues \u00b6 Sometimes the login via SSO - requires you to again add your existing data, and adding it again throws an error because its already saved in the database. you can remove the entry and try again to login with SSO. keycloak = # DELETE FROM federated_identity WHERE federated_username='mb1990';","title":"Keycloak Database"},{"location":"database/keycloak-db/#keycloak-database","text":"This Database is used for Keycloak.","title":"Keycloak Database"},{"location":"database/keycloak-db/#initialize-database","text":"","title":"Initialize database"},{"location":"database/keycloak-db/#access-database","text":"ssh root@DB_HOST.com psql -U postgres","title":"Access Database"},{"location":"database/keycloak-db/#create-required-database-and-user","text":"# create keycloak user create user keycloak with password '********' ; # create keycloak database create database keycloak with owner keycloak ; Note: The database initializes automatically.","title":"Create required Database and User"},{"location":"database/keycloak-db/#backup-database","text":"To backup the database we will use the pg_dump , make sure this plugin is installed. # -U user -d database pg_dump -U keycloak -d keycloak > keycloak.sql","title":"Backup Database"},{"location":"database/keycloak-db/#issues","text":"Sometimes the login via SSO - requires you to again add your existing data, and adding it again throws an error because its already saved in the database. you can remove the entry and try again to login with SSO. keycloak = # DELETE FROM federated_identity WHERE federated_username='mb1990';","title":"Issues"},{"location":"database/main/","text":"Databases \u00b6 CyVerse is using PostgreSQL as its database. Most of These database dedicated to the discovery environment of Cyverse, which is used for multiple services such as: Database Owner Auto Init Auto Migrate de Database de no no Notifications Database de no no Metadata Database de no no DE Releases de no no Grouper Database grouper yes ? QMS Database de configurable configurable Unleash Database unleash_user yes ? Keycloak Database keycloak yes ? Portal Database portal_db_reader no no NOTE: permissions database has been merged with DE database. Setup \u00b6 On this paragraph we will cover first and necessary steps to configure the database. ~postgres/12/data/pg_hba.conf \u00b6 IPv4 local connections: Add IP or IP range of kubernetes worker node, that requires connection to this database. TYPE DATABASE USER ADDRESS METHOD host all all * /32 md5 ~postgres/12/data/postgresql.conf \u00b6 # vi ~postgres/12/data/postgresql.conf listen_addresses = '*' # what IP address(es) to listen on; This database dedicated to the iRODS iRODS Database Database Owner Auto Init Auto Migrate iRODS Database icat_reader & irods no no","title":"Databases"},{"location":"database/main/#databases","text":"CyVerse is using PostgreSQL as its database. Most of These database dedicated to the discovery environment of Cyverse, which is used for multiple services such as: Database Owner Auto Init Auto Migrate de Database de no no Notifications Database de no no Metadata Database de no no DE Releases de no no Grouper Database grouper yes ? QMS Database de configurable configurable Unleash Database unleash_user yes ? Keycloak Database keycloak yes ? Portal Database portal_db_reader no no NOTE: permissions database has been merged with DE database.","title":"Databases"},{"location":"database/main/#setup","text":"On this paragraph we will cover first and necessary steps to configure the database.","title":"Setup"},{"location":"database/main/#postgres12datapg_hbaconf","text":"IPv4 local connections: Add IP or IP range of kubernetes worker node, that requires connection to this database. TYPE DATABASE USER ADDRESS METHOD host all all * /32 md5","title":"~postgres/12/data/pg_hba.conf"},{"location":"database/main/#postgres12datapostgresqlconf","text":"# vi ~postgres/12/data/postgresql.conf listen_addresses = '*' # what IP address(es) to listen on; This database dedicated to the iRODS iRODS Database Database Owner Auto Init Auto Migrate iRODS Database icat_reader & irods no no","title":"~postgres/12/data/postgresql.conf"},{"location":"database/metadata-db/","text":"Metadata Database \u00b6 This Database is used for CyVerse Metadata service. Initialize database \u00b6 Access Database \u00b6 ssh root@DB_HOST.com psql -U postgres Create required Database \u00b6 # create metadata database with de user create database metadata with owner de ; Add required extensions \u00b6 # psql -U postgres \\c metadata create extension \"uuid-ossp\" ; create extension \"moddatetime\" ; create extension \"btree_gist\" ; Populate / Migrate the Database \u00b6 Ensure the following before running the command: - Docker is installed on your host machine. - The host has network access to the target PostgreSQL database. Clone the migration repository: git clone https://github.com/cyverse-austria/metadata-db.git Run the migrations using Docker: docker run --rm \\ -v $( pwd ) /metadata-db/migrations:/migrations \\ --network host \\ migrate/migrate \\ --database \"postgres://de: $DE_PASSWORD @ $DB_HOST /metadata?sslmode=disable\" \\ -path /migrations \\ up","title":"Metadata Database"},{"location":"database/metadata-db/#metadata-database","text":"This Database is used for CyVerse Metadata service.","title":"Metadata Database"},{"location":"database/metadata-db/#initialize-database","text":"","title":"Initialize database"},{"location":"database/metadata-db/#access-database","text":"ssh root@DB_HOST.com psql -U postgres","title":"Access Database"},{"location":"database/metadata-db/#create-required-database","text":"# create metadata database with de user create database metadata with owner de ;","title":"Create required Database"},{"location":"database/metadata-db/#add-required-extensions","text":"# psql -U postgres \\c metadata create extension \"uuid-ossp\" ; create extension \"moddatetime\" ; create extension \"btree_gist\" ;","title":"Add required extensions"},{"location":"database/metadata-db/#populate-migrate-the-database","text":"Ensure the following before running the command: - Docker is installed on your host machine. - The host has network access to the target PostgreSQL database. Clone the migration repository: git clone https://github.com/cyverse-austria/metadata-db.git Run the migrations using Docker: docker run --rm \\ -v $( pwd ) /metadata-db/migrations:/migrations \\ --network host \\ migrate/migrate \\ --database \"postgres://de: $DE_PASSWORD @ $DB_HOST /metadata?sslmode=disable\" \\ -path /migrations \\ up","title":"Populate / Migrate the Database"},{"location":"database/notifications-db/","text":"Notifications Database \u00b6 This Database is used for CyVerse notifications service. Initialize database \u00b6 Access Database \u00b6 ssh root@DB_HOST.com psql -U postgres Create required Database \u00b6 # create notifications database with de owner. create database notifications with owner de ; Add required extensions \u00b6 # psql -U postgres \\c notifications create extension \"uuid-ossp\" ; create extension \"moddatetime\" ; create extension \"btree_gist\" ; Populate / Migrate the Database \u00b6 Ensure the following before running the command: - Docker is installed on your host machine. - The host has network access to the target PostgreSQL database. Clone the migration repository: git clone https://github.com/cyverse-austria/notifications-db.git Run the migrations using Docker: docker run --rm \\ -v $( pwd ) /notifications-db/migrations:/migrations \\ --network host \\ migrate/migrate \\ --database \"postgres://de: $DE_PASSWORD @ $DB_HOST /notifications?sslmode=disable\" \\ -path /migrations \\ up","title":"Notifications Database"},{"location":"database/notifications-db/#notifications-database","text":"This Database is used for CyVerse notifications service.","title":"Notifications Database"},{"location":"database/notifications-db/#initialize-database","text":"","title":"Initialize database"},{"location":"database/notifications-db/#access-database","text":"ssh root@DB_HOST.com psql -U postgres","title":"Access Database"},{"location":"database/notifications-db/#create-required-database","text":"# create notifications database with de owner. create database notifications with owner de ;","title":"Create required Database"},{"location":"database/notifications-db/#add-required-extensions","text":"# psql -U postgres \\c notifications create extension \"uuid-ossp\" ; create extension \"moddatetime\" ; create extension \"btree_gist\" ;","title":"Add required extensions"},{"location":"database/notifications-db/#populate-migrate-the-database","text":"Ensure the following before running the command: - Docker is installed on your host machine. - The host has network access to the target PostgreSQL database. Clone the migration repository: git clone https://github.com/cyverse-austria/notifications-db.git Run the migrations using Docker: docker run --rm \\ -v $( pwd ) /notifications-db/migrations:/migrations \\ --network host \\ migrate/migrate \\ --database \"postgres://de: $DE_PASSWORD @ $DB_HOST /notifications?sslmode=disable\" \\ -path /migrations \\ up","title":"Populate / Migrate the Database"},{"location":"database/portal-db/","text":"Portal Database \u00b6 This Database is used for CyVerse User portal Service. Initialize database \u00b6 Access Database \u00b6 ssh root@DB_HOST.com psql -U postgres Create required Database and User \u00b6 # create user create user portal_db_reader with password '********' ; # create portal database with the owner portal_db_reader create database portal with owner portal_db_reader ; ## Grant user to member of postgres # psql -U postgres GRANT postgres TO portal_db_reader ; Populate / Migrate the Database \u00b6 Ensure the following before running the command: Docker is installed on your host machine. The host has network access to the target PostgreSQL database. Clone the migration repository: git clone https://github.com/cyverse-de/portal2 Run the migrations using Docker: docker run --rm \\ -v $( pwd ) /portal2/migrations:/migrations \\ --network host \\ migrate/migrate \\ --database \"postgres:// $PORTAL_USER : $PORTAL_PASSWORD @ $DB_HOST /portal?sslmode=disable\" \\ -path /migrations \\ up Extra \u00b6 Give Admin privilege to a user \u00b6 --update is_superuser UPDATE account_user SET is_superuser = true WHERE username = 'USERNAME' ; ---update is_staff UPDATE account_user SET is_staff = true WHERE username = 'USERNAME' ; Verify User email \u00b6 -- check if its verified select has_verified_email from account_user where username = 'USERNAME' ; --Verify email UPDATE account_user SET has_verified_email = true WHERE username = 'USERNAME' ; Insert Services \u00b6 by default the table api_service will be empty, in order to add a new service, we will to INSERT INTO the database. For this we need to add first the api_servicemaintainer column, and then api_service - as an example we will be adding a DE(Discovery environment) service. -- check the tables content select * from api_servicemaintainer ; select * from api_service ; -- First Insert into api_servicemaintainer -- save the id of this record - which we will use in the next step INSERT INTO api_servicemaintainer ( name , website_url , created_at , updated_at ) VALUES ( 'CyVerse' , 'https://cyverse.tugraz.at' , now (), now ()); -- Second Insert into api_service INSERT INTO api_service ( name , description , about , service_url , is_public , icon_url , created_at , updated_at , service_maintainer_id , approval_key , subtitle ) VALUES ( 'Discovery Environment' , 'Use hundreds of bioinformatics apps and manage data in the CyVerse Data Store from a simple web interface' , 'By providing a consistent user interface for access to the tools and computing resources needed for specialized scientific analyses, the Discovery Environment facilitates data exploration and scientific discovery.\\r' , 'https://de.cyverse.at/de' , true , 'https://user.cyverse.at/assets/images/de.png' , now (), now (), 3 , 'DISCOVERY_ENVIRONMENT' , '' ); Insert Restricted Username (PENDING) \u00b6 INSERT INTO account_restrictedusername ( username , created_at , updated_at ) VALUES ( 'username' , now (), now ()); make sure session Table exist \u00b6 CREATE TABLE public.session ( sid character varying NOT NULL, sess json NOT NULL, expire timestamp ( 6 ) without time zone NOT NULL ) ; ALTER TABLE public.session OWNER TO portal ; CREATE INDEX \"IDX_session_expire\" ON public.session USING btree ( expire ) ; Import GRID institutions \u00b6 download required grid file \u00b6 Official website # download grid wget https://digitalscience.figshare.com/ndownloader/files/30895309 # unzip unzip 30895309 import to the database \u00b6 For imorting this grid file we will use the script from portal2/scripts/import_grid_institutions.py . ./import_grid_institutions.py --host DB_HOST --user portal_db_reader --database portal grid.csv Populate these Tables \u00b6 These sql files can be found here . psql -U portal_db_reader -d portal -f ./account_country.sql psql -U portal_db_reader -d portal -f ./account_region.sql psql -U portal_db_reader -d portal -f ./account_gender.sql psql -U portal_db_reader -d portal -f ./account_occupation.sql psql -U portal_db_reader -d portal -f ./account_ethnicity.sql psql -U portal_db_reader -d portal -f ./account_fundingagency.sql psql -U portal_db_reader -d portal -f ./account_awarechannel.sql psql -U portal_db_reader -d portal -f ./account_researcharea.sql Creating an iRODS Account for the Portal Service \u00b6 To create an iRODS account for the Portal service, please refer to the documentation: \ud83d\udd17 Create iRODS Account for Portal","title":"Portal Database"},{"location":"database/portal-db/#portal-database","text":"This Database is used for CyVerse User portal Service.","title":"Portal Database"},{"location":"database/portal-db/#initialize-database","text":"","title":"Initialize database"},{"location":"database/portal-db/#access-database","text":"ssh root@DB_HOST.com psql -U postgres","title":"Access Database"},{"location":"database/portal-db/#create-required-database-and-user","text":"# create user create user portal_db_reader with password '********' ; # create portal database with the owner portal_db_reader create database portal with owner portal_db_reader ; ## Grant user to member of postgres # psql -U postgres GRANT postgres TO portal_db_reader ;","title":"Create required Database and User"},{"location":"database/portal-db/#populate-migrate-the-database","text":"Ensure the following before running the command: Docker is installed on your host machine. The host has network access to the target PostgreSQL database. Clone the migration repository: git clone https://github.com/cyverse-de/portal2 Run the migrations using Docker: docker run --rm \\ -v $( pwd ) /portal2/migrations:/migrations \\ --network host \\ migrate/migrate \\ --database \"postgres:// $PORTAL_USER : $PORTAL_PASSWORD @ $DB_HOST /portal?sslmode=disable\" \\ -path /migrations \\ up","title":"Populate / Migrate the Database"},{"location":"database/portal-db/#extra","text":"","title":"Extra"},{"location":"database/portal-db/#give-admin-privilege-to-a-user","text":"--update is_superuser UPDATE account_user SET is_superuser = true WHERE username = 'USERNAME' ; ---update is_staff UPDATE account_user SET is_staff = true WHERE username = 'USERNAME' ;","title":"Give Admin privilege to a user"},{"location":"database/portal-db/#verify-user-email","text":"-- check if its verified select has_verified_email from account_user where username = 'USERNAME' ; --Verify email UPDATE account_user SET has_verified_email = true WHERE username = 'USERNAME' ;","title":"Verify User email"},{"location":"database/portal-db/#insert-services","text":"by default the table api_service will be empty, in order to add a new service, we will to INSERT INTO the database. For this we need to add first the api_servicemaintainer column, and then api_service - as an example we will be adding a DE(Discovery environment) service. -- check the tables content select * from api_servicemaintainer ; select * from api_service ; -- First Insert into api_servicemaintainer -- save the id of this record - which we will use in the next step INSERT INTO api_servicemaintainer ( name , website_url , created_at , updated_at ) VALUES ( 'CyVerse' , 'https://cyverse.tugraz.at' , now (), now ()); -- Second Insert into api_service INSERT INTO api_service ( name , description , about , service_url , is_public , icon_url , created_at , updated_at , service_maintainer_id , approval_key , subtitle ) VALUES ( 'Discovery Environment' , 'Use hundreds of bioinformatics apps and manage data in the CyVerse Data Store from a simple web interface' , 'By providing a consistent user interface for access to the tools and computing resources needed for specialized scientific analyses, the Discovery Environment facilitates data exploration and scientific discovery.\\r' , 'https://de.cyverse.at/de' , true , 'https://user.cyverse.at/assets/images/de.png' , now (), now (), 3 , 'DISCOVERY_ENVIRONMENT' , '' );","title":"Insert Services"},{"location":"database/portal-db/#insert-restricted-username-pending","text":"INSERT INTO account_restrictedusername ( username , created_at , updated_at ) VALUES ( 'username' , now (), now ());","title":"Insert Restricted Username (PENDING)"},{"location":"database/portal-db/#make-sure-session-table-exist","text":"CREATE TABLE public.session ( sid character varying NOT NULL, sess json NOT NULL, expire timestamp ( 6 ) without time zone NOT NULL ) ; ALTER TABLE public.session OWNER TO portal ; CREATE INDEX \"IDX_session_expire\" ON public.session USING btree ( expire ) ;","title":"make sure session Table exist"},{"location":"database/portal-db/#import-grid-institutions","text":"","title":"Import GRID institutions"},{"location":"database/portal-db/#download-required-grid-file","text":"Official website # download grid wget https://digitalscience.figshare.com/ndownloader/files/30895309 # unzip unzip 30895309","title":"download required grid file"},{"location":"database/portal-db/#import-to-the-database","text":"For imorting this grid file we will use the script from portal2/scripts/import_grid_institutions.py . ./import_grid_institutions.py --host DB_HOST --user portal_db_reader --database portal grid.csv","title":"import to the database"},{"location":"database/portal-db/#populate-these-tables","text":"These sql files can be found here . psql -U portal_db_reader -d portal -f ./account_country.sql psql -U portal_db_reader -d portal -f ./account_region.sql psql -U portal_db_reader -d portal -f ./account_gender.sql psql -U portal_db_reader -d portal -f ./account_occupation.sql psql -U portal_db_reader -d portal -f ./account_ethnicity.sql psql -U portal_db_reader -d portal -f ./account_fundingagency.sql psql -U portal_db_reader -d portal -f ./account_awarechannel.sql psql -U portal_db_reader -d portal -f ./account_researcharea.sql","title":"Populate these Tables"},{"location":"database/portal-db/#creating-an-irods-account-for-the-portal-service","text":"To create an iRODS account for the Portal service, please refer to the documentation: \ud83d\udd17 Create iRODS Account for Portal","title":"Creating an iRODS Account for the Portal Service"},{"location":"database/qms-db/","text":"QMS Database \u00b6 This Database is used for CyVerse QMS service. Initialize database \u00b6 Access vm \u00b6 ssh root@DB_HOST.com Create required Database \u00b6 # create qms database with de owner create database qms with owner de ; Add required extensions \u00b6 # psql -U postgres \\c qms create extension \"uuid-ossp\" ; create extension \"moddatetime\" ; create extension \"btree_gist\" ; create extension \"insert_username\" ; Populate / Migrate the Database \u00b6 Ensure the following before running the command: - Docker is installed on your host machine. - The host has network access to the target PostgreSQL database. Clone the migration repository: git clone https://github.com/cyverse-austria/qms.git Run the migrations using Docker: docker run --rm \\ -v $( pwd ) /qms/migrations:/migrations \\ --network host \\ migrate/migrate \\ --database \"postgres://de: $DE_PASSWORD @ $DB_HOST /qms?sslmode=disable\" \\ -path /migrations \\ up Note: Replace $DE_USER, $DE_PASSWORD, and $DB_HOST with the appropriate environment variables or values.","title":"QMS Database"},{"location":"database/qms-db/#qms-database","text":"This Database is used for CyVerse QMS service.","title":"QMS Database"},{"location":"database/qms-db/#initialize-database","text":"","title":"Initialize database"},{"location":"database/qms-db/#access-vm","text":"ssh root@DB_HOST.com","title":"Access vm"},{"location":"database/qms-db/#create-required-database","text":"# create qms database with de owner create database qms with owner de ;","title":"Create required Database"},{"location":"database/qms-db/#add-required-extensions","text":"# psql -U postgres \\c qms create extension \"uuid-ossp\" ; create extension \"moddatetime\" ; create extension \"btree_gist\" ; create extension \"insert_username\" ;","title":"Add required extensions"},{"location":"database/qms-db/#populate-migrate-the-database","text":"Ensure the following before running the command: - Docker is installed on your host machine. - The host has network access to the target PostgreSQL database. Clone the migration repository: git clone https://github.com/cyverse-austria/qms.git Run the migrations using Docker: docker run --rm \\ -v $( pwd ) /qms/migrations:/migrations \\ --network host \\ migrate/migrate \\ --database \"postgres://de: $DE_PASSWORD @ $DB_HOST /qms?sslmode=disable\" \\ -path /migrations \\ up Note: Replace $DE_USER, $DE_PASSWORD, and $DB_HOST with the appropriate environment variables or values.","title":"Populate / Migrate the Database"},{"location":"database/unleash-db/","text":"Unleash Database \u00b6 This Database is used for CyVerse Unleash service. Initialize database \u00b6 Access vm \u00b6 ssh root@DB_HOST.com Create required Database and User \u00b6 # create unleash user create user unleash with password '********' ; # create unleash databased create database unleash with owner unleash ; Add required extensions \u00b6 # psql -U postgres \\c unleash create extension \"uuid-ossp\" ; create extension \"moddatetime\" ; create extension \"btree_gist\" ; Note: The database initializes automatically.","title":"Unleash Database"},{"location":"database/unleash-db/#unleash-database","text":"This Database is used for CyVerse Unleash service.","title":"Unleash Database"},{"location":"database/unleash-db/#initialize-database","text":"","title":"Initialize database"},{"location":"database/unleash-db/#access-vm","text":"ssh root@DB_HOST.com","title":"Access vm"},{"location":"database/unleash-db/#create-required-database-and-user","text":"# create unleash user create user unleash with password '********' ; # create unleash databased create database unleash with owner unleash ;","title":"Create required Database and User"},{"location":"database/unleash-db/#add-required-extensions","text":"# psql -U postgres \\c unleash create extension \"uuid-ossp\" ; create extension \"moddatetime\" ; create extension \"btree_gist\" ; Note: The database initializes automatically.","title":"Add required extensions"},{"location":"deployments/CertManager/","text":"install & configure cert-manager via k8s-resources \u00b6 kubectl create namespace cert-manager # install cert-manager custom kubectl apply -f resources/addons/cert-manager/cert-manager.custom.yaml # apply issuer kubectl apply -f resources/addons/cert-manager/issuers.yaml -n discover","title":"Cert-Manager"},{"location":"deployments/CertManager/#install-configure-cert-manager-via-k8s-resources","text":"kubectl create namespace cert-manager # install cert-manager custom kubectl apply -f resources/addons/cert-manager/cert-manager.custom.yaml # apply issuer kubectl apply -f resources/addons/cert-manager/issuers.yaml -n discover","title":"install &amp; configure cert-manager via k8s-resources"},{"location":"deployments/DiscoveryEnvironment/","text":"Discovery Environment UI \u00b6 Deploying DiscoveryEnvironment UI which is nginx proxying all the services to the DiscoveryEnvironment of cyverse. Preq \u00b6 Make sure you have the harbor-registry-credentials secrets in your NAMESPACE, see also k8s-resources . Make sure you have de-nginx-tls secret created, also see k8s-resources . Make sure your Haproxy has the CA cert, Add /docker-tugraz-data/ca/ca.pem to HAPROXY_DOMAIN :/etc/ssl/certs/ca-bundle.crt` Deploying \u00b6 For deploying Discovery environment we are using the manifest files from the k8s-resources . Change hardcoded \u00b6 If you are using a diffrent domain instead of cyverse.tugraz.at , e.g. cyverse.at - change these two files. /k8s-resources/resources/kustomize/de-nginx/base/nginx.conf - server_name ~^[^.]+[.]cyverse[.]tugraz[.]at$; + server_name ~^[^.]+[.]cyverse[.]at$; k8s-resources/resources/kustomize/de-nginx/base/kustomization.yaml - namespace: prod + namespace: discover Deploy \u00b6 ## For prod env kubectl apply -k resources/kustomize/de-nginx/overlays/prod/ -n prod kubectl apply -f resources/services/tugraz.yml -n prod ## For discover env # kubectl apply -k resources/kustomize/de-nginx/overlays/prod/ -n discover # kubectl apply -f resources/services/tugraz.yml -n discover","title":"DiscoveryEnvironment"},{"location":"deployments/DiscoveryEnvironment/#discovery-environment-ui","text":"Deploying DiscoveryEnvironment UI which is nginx proxying all the services to the DiscoveryEnvironment of cyverse.","title":"Discovery Environment UI"},{"location":"deployments/DiscoveryEnvironment/#preq","text":"Make sure you have the harbor-registry-credentials secrets in your NAMESPACE, see also k8s-resources . Make sure you have de-nginx-tls secret created, also see k8s-resources . Make sure your Haproxy has the CA cert, Add /docker-tugraz-data/ca/ca.pem to HAPROXY_DOMAIN :/etc/ssl/certs/ca-bundle.crt`","title":"Preq"},{"location":"deployments/DiscoveryEnvironment/#deploying","text":"For deploying Discovery environment we are using the manifest files from the k8s-resources .","title":"Deploying"},{"location":"deployments/DiscoveryEnvironment/#change-hardcoded","text":"If you are using a diffrent domain instead of cyverse.tugraz.at , e.g. cyverse.at - change these two files. /k8s-resources/resources/kustomize/de-nginx/base/nginx.conf - server_name ~^[^.]+[.]cyverse[.]tugraz[.]at$; + server_name ~^[^.]+[.]cyverse[.]at$; k8s-resources/resources/kustomize/de-nginx/base/kustomization.yaml - namespace: prod + namespace: discover","title":"Change hardcoded"},{"location":"deployments/DiscoveryEnvironment/#deploy","text":"## For prod env kubectl apply -k resources/kustomize/de-nginx/overlays/prod/ -n prod kubectl apply -f resources/services/tugraz.yml -n prod ## For discover env # kubectl apply -k resources/kustomize/de-nginx/overlays/prod/ -n discover # kubectl apply -f resources/services/tugraz.yml -n discover","title":"Deploy"},{"location":"deployments/Nats/","text":"Install nats \u00b6 Here are some links for docs. * Docs for nats in k8s. * nats helm chart * CyverseUS howto install & configure nats (discover) # create certificates via cert-manager kubectl apply -n discover -f resources/addons/nats/env/discover/nats-certs.yaml # apply nodeport service kubectl apply -n discover -f resources/addons/nats/env/discover/nats-service.yaml # install nats via helm helm repo add nats https://nats-io.github.io/k8s/helm/charts/ helm repo update helm install nats nats/nats -n discover --values resources/addons/nats/values.yaml Create nats-services-creds secret \u00b6 For creating the nats-services-creds we need to take these steps: exec inside the nats-box pod and run the followings. See Also CyverseUS docs ~ # nsc init ? enter a configuration directory : ` /nsc/nats/nsc/stores ` ? Select an operator : ` Create Operator ` ? name your operator, account and user : ` de ` ~ # nsc add operator -n cyverse --sys [ OK ] generated and stored operator key \"OANHTCLQLGTQFVRPNBTJ7HD56HTKMORLSLBZYUD7ETQAB3EQUBYFRD7J\" [ OK ] added operator \"cyverse\" [ OK ] When running your own nats-server, make sure they run at least version 2 .2.0 [ OK ] created system_account: name:SYS id:AA3TP5UNGFT3Z3CZAQDUUKYTWBMCBWMPQQ7Z4BVFG43ZBNRKQUJHBWJF [ OK ] created system account user: name:sys id:UCSUROZYTQZPVC6NRFUMLOLZ472ZFCUREKVN6J7FOUGXYCU4BCVZ43J3 [ OK ] system account user creds file stored in ` /nsc/nkeys/creds/cyverse/SYS/sys.creds ` ~ # nsc generate nkey -o --store OBH2PGMFLUSYDTA3OUTSOJT5LSRIY2CJU6MCEMJJMAMULWZMYHSEISQN operator key stored /nsc/nkeys/keys/O/BH/OBH2PGMFLUSYDTA3OUTSOJT5LSRIY2CJU6MCEMJJMAMULWZMYHSEISQN.nk ~ # nsc edit operator --sk OBH2PGMFLUSYDTA3OUTSOJT5LSRIY2CJU6MCEMJJMAMULWZMYHSEISQN [ OK ] added signing key \"OBH2PGMFLUSYDTA3OUTSOJT5LSRIY2CJU6MCEMJJMAMULWZMYHSEISQN\" [ OK ] edited operator \"cyverse\" ~ # nsc add account -n de -K OANHTCLQLGTQFVRPNBTJ7HD56HTKMORLSLBZYUD7ETQAB3EQUBYFRD7J [ OK ] generated and stored account key \"AAPA2QMALWDT7MQOITMTTOULTVMUJM2CW4UGPBKECAJDYCYBK6JZNH2Z\" [ OK ] added account \"de\" ~ # nsc generate nkey -a --store ABDVLFKHFWYBHJWUNC35S3VA4YGCW2SVD6MU2RSZBOJD42SJFXZLY372 account key stored /nsc/nkeys/keys/A/BD/ABDVLFKHFWYBHJWUNC35S3VA4YGCW2SVD6MU2RSZBOJD42SJFXZLY372.nk ~ # nsc edit account -n de --sk ABDVLFKHFWYBHJWUNC35S3VA4YGCW2SVD6MU2RSZBOJD42SJFXZLY372 [ OK ] added signing key \"ABDVLFKHFWYBHJWUNC35S3VA4YGCW2SVD6MU2RSZBOJD42SJFXZLY372\" [ OK ] edited account \"de\" ~ # nsc add user --account de --name services -K ABDVLFKHFWYBHJWUNC35S3VA4YGCW2SVD6MU2RSZBOJD42SJFXZLY372 [ OK ] generated and stored user key \"UAYRAHG5GXMA4DX4S46HYWWPIK5MWO64RRHXBGUDB32WXCOW6EKQFJUN\" [ OK ] generated user creds file ` /nsc/nkeys/creds/cyverse/de/services.creds ` [ OK ] added user \"services\" to account \"de\" ~ # cat /nsc/nkeys/creds/cyverse/de/services.creds copy secret file from the nats-box pod & create from it a secret # copy the cred file from the pod kubectl -n discover cp nats-box-6cdf5f4bb-q9s5m:/nsc/nkeys/creds/cyverse/de/services.creds services.creds # create the secret from a file which we have created kubectl -n discover create secret generic nats-services-creds --from-file services.creds","title":"Nats"},{"location":"deployments/Nats/#install-nats","text":"Here are some links for docs. * Docs for nats in k8s. * nats helm chart * CyverseUS howto install & configure nats (discover) # create certificates via cert-manager kubectl apply -n discover -f resources/addons/nats/env/discover/nats-certs.yaml # apply nodeport service kubectl apply -n discover -f resources/addons/nats/env/discover/nats-service.yaml # install nats via helm helm repo add nats https://nats-io.github.io/k8s/helm/charts/ helm repo update helm install nats nats/nats -n discover --values resources/addons/nats/values.yaml","title":"Install nats"},{"location":"deployments/Nats/#create-nats-services-creds-secret","text":"For creating the nats-services-creds we need to take these steps: exec inside the nats-box pod and run the followings. See Also CyverseUS docs ~ # nsc init ? enter a configuration directory : ` /nsc/nats/nsc/stores ` ? Select an operator : ` Create Operator ` ? name your operator, account and user : ` de ` ~ # nsc add operator -n cyverse --sys [ OK ] generated and stored operator key \"OANHTCLQLGTQFVRPNBTJ7HD56HTKMORLSLBZYUD7ETQAB3EQUBYFRD7J\" [ OK ] added operator \"cyverse\" [ OK ] When running your own nats-server, make sure they run at least version 2 .2.0 [ OK ] created system_account: name:SYS id:AA3TP5UNGFT3Z3CZAQDUUKYTWBMCBWMPQQ7Z4BVFG43ZBNRKQUJHBWJF [ OK ] created system account user: name:sys id:UCSUROZYTQZPVC6NRFUMLOLZ472ZFCUREKVN6J7FOUGXYCU4BCVZ43J3 [ OK ] system account user creds file stored in ` /nsc/nkeys/creds/cyverse/SYS/sys.creds ` ~ # nsc generate nkey -o --store OBH2PGMFLUSYDTA3OUTSOJT5LSRIY2CJU6MCEMJJMAMULWZMYHSEISQN operator key stored /nsc/nkeys/keys/O/BH/OBH2PGMFLUSYDTA3OUTSOJT5LSRIY2CJU6MCEMJJMAMULWZMYHSEISQN.nk ~ # nsc edit operator --sk OBH2PGMFLUSYDTA3OUTSOJT5LSRIY2CJU6MCEMJJMAMULWZMYHSEISQN [ OK ] added signing key \"OBH2PGMFLUSYDTA3OUTSOJT5LSRIY2CJU6MCEMJJMAMULWZMYHSEISQN\" [ OK ] edited operator \"cyverse\" ~ # nsc add account -n de -K OANHTCLQLGTQFVRPNBTJ7HD56HTKMORLSLBZYUD7ETQAB3EQUBYFRD7J [ OK ] generated and stored account key \"AAPA2QMALWDT7MQOITMTTOULTVMUJM2CW4UGPBKECAJDYCYBK6JZNH2Z\" [ OK ] added account \"de\" ~ # nsc generate nkey -a --store ABDVLFKHFWYBHJWUNC35S3VA4YGCW2SVD6MU2RSZBOJD42SJFXZLY372 account key stored /nsc/nkeys/keys/A/BD/ABDVLFKHFWYBHJWUNC35S3VA4YGCW2SVD6MU2RSZBOJD42SJFXZLY372.nk ~ # nsc edit account -n de --sk ABDVLFKHFWYBHJWUNC35S3VA4YGCW2SVD6MU2RSZBOJD42SJFXZLY372 [ OK ] added signing key \"ABDVLFKHFWYBHJWUNC35S3VA4YGCW2SVD6MU2RSZBOJD42SJFXZLY372\" [ OK ] edited account \"de\" ~ # nsc add user --account de --name services -K ABDVLFKHFWYBHJWUNC35S3VA4YGCW2SVD6MU2RSZBOJD42SJFXZLY372 [ OK ] generated and stored user key \"UAYRAHG5GXMA4DX4S46HYWWPIK5MWO64RRHXBGUDB32WXCOW6EKQFJUN\" [ OK ] generated user creds file ` /nsc/nkeys/creds/cyverse/de/services.creds ` [ OK ] added user \"services\" to account \"de\" ~ # cat /nsc/nkeys/creds/cyverse/de/services.creds copy secret file from the nats-box pod & create from it a secret # copy the cred file from the pod kubectl -n discover cp nats-box-6cdf5f4bb-q9s5m:/nsc/nkeys/creds/cyverse/de/services.creds services.creds # create the secret from a file which we have created kubectl -n discover create secret generic nats-services-creds --from-file services.creds","title":"Create nats-services-creds secret"},{"location":"deployments/RabbitMQ/","text":"AMQP \u00b6 We are using ansible playbooks to install and configure AMQP server. Reindex RabbitMQ jobs \u00b6 access the vm where the RabbitMQ is installed. \u00b6 ssh root@RABBITMQ_HOST Configure rabbitmqadmin \u00b6 This step you have to do only once, if the rabbitmqadmin is not present. mkdir adm cd adm wget http://localhost:15672/cli/rabbitmqadmin chmod +x rabbitmqadmin Commands \u00b6 QA \u00b6 # check status systemctl status rabbitmq-server.service -l ## add your password to a temp var read -s PASSWORD && export PASSWORD # list exchange ./rabbitmqadmin -V /cyverse/de list exchanges -u cyverse -p $PASSWORD # publish the message to reindex all ./rabbitmqadmin publish -V /cyverse/de -u cyverse -p $PASSWORD exchange = de routing_key = index.all payload = \"\" # restart services kubectl rollout restart deployment infosquito2 -n qa kubectl rollout restart deployment search -n qa # IF not deployed # ./deploy.py -Bn qa -p infosquito2 search -C PROD \u00b6 ## add your password to a temp var read -s PASSWORD && export PASSWORD # list ./rabbitmqadmin -V /tugraz/de list exchanges -u tugraz -p $PASSWORD # publish the message to reindex all ./rabbitmqadmin publish -V /tugraz/de -u tugraz -p $PASSWORD exchange = de routing_key = index.all payload = \"\"","title":"RabbitMQ"},{"location":"deployments/RabbitMQ/#amqp","text":"We are using ansible playbooks to install and configure AMQP server.","title":"AMQP"},{"location":"deployments/RabbitMQ/#reindex-rabbitmq-jobs","text":"","title":"Reindex RabbitMQ jobs"},{"location":"deployments/RabbitMQ/#access-the-vm-where-the-rabbitmq-is-installed","text":"ssh root@RABBITMQ_HOST","title":"access the vm where the RabbitMQ is installed."},{"location":"deployments/RabbitMQ/#configure-rabbitmqadmin","text":"This step you have to do only once, if the rabbitmqadmin is not present. mkdir adm cd adm wget http://localhost:15672/cli/rabbitmqadmin chmod +x rabbitmqadmin","title":"Configure rabbitmqadmin"},{"location":"deployments/RabbitMQ/#commands","text":"","title":"Commands"},{"location":"deployments/RabbitMQ/#qa","text":"# check status systemctl status rabbitmq-server.service -l ## add your password to a temp var read -s PASSWORD && export PASSWORD # list exchange ./rabbitmqadmin -V /cyverse/de list exchanges -u cyverse -p $PASSWORD # publish the message to reindex all ./rabbitmqadmin publish -V /cyverse/de -u cyverse -p $PASSWORD exchange = de routing_key = index.all payload = \"\" # restart services kubectl rollout restart deployment infosquito2 -n qa kubectl rollout restart deployment search -n qa # IF not deployed # ./deploy.py -Bn qa -p infosquito2 search -C","title":"QA"},{"location":"deployments/RabbitMQ/#prod","text":"## add your password to a temp var read -s PASSWORD && export PASSWORD # list ./rabbitmqadmin -V /tugraz/de list exchanges -u tugraz -p $PASSWORD # publish the message to reindex all ./rabbitmqadmin publish -V /tugraz/de -u tugraz -p $PASSWORD exchange = de routing_key = index.all payload = \"\"","title":"PROD"},{"location":"deployments/elasticsearch/","text":"Elasticsearch \u00b6 Currently deployed version: docker.elastic.co/elasticsearch/elasticsearch:5.6.14 Deploying the statefulsets Elasticsearch cluster on kubernetes. Deploying \u00b6 Preq \u00b6 clone the repo see also k8s-resources cd /k8s-resources In case you have limited resources change these: vi resources/addons/elasticsearch/elasticsearch.yml replicas : 2 resources : requests : memory : \"4Gi\" limits : memory : \"4Gi\" - name : ES_JAVA_OPTS value : \"-Xms2g -Xmx2g\" Deploy \u00b6 # deploy ES on prod env kubectl apply -n prod -f resources/addons/elasticsearch/elasticsearch.yml ## deploy ES on discover env # kubectl apply -n discover -f resources/addons/elasticsearch/elasticsearch.yml Indexing \u00b6 Preq \u00b6 Save below json to a file settings.json , we will use this file to index our elasticsearch. { \"mappings\" : { \"file\" : { \"properties\" : { \"creator\" : { \"type\" : \"keyword\" }, \"dateCreated\" : { \"type\" : \"date\" }, \"dateModified\" : { \"type\" : \"date\" }, \"fileSize\" : { \"type\" : \"long\" }, \"fileType\" : { \"type\" : \"keyword\" }, \"id\" : { \"type\" : \"keyword\" }, \"label\" : { \"type\" : \"text\" , \"fields\" : { \"keyword\" : { \"type\" : \"keyword\" } }, \"analyzer\" : \"irods_entity\" }, \"metadata\" : { \"type\" : \"nested\" , \"properties\" : { \"attribute\" : { \"type\" : \"text\" , \"analyzer\" : \"irods_entity\" }, \"unit\" : { \"type\" : \"keyword\" }, \"value\" : { \"type\" : \"text\" } } }, \"path\" : { \"type\" : \"text\" , \"fields\" : { \"keyword\" : { \"type\" : \"keyword\" } }, \"analyzer\" : \"irods_path\" }, \"userPermissions\" : { \"type\" : \"nested\" , \"properties\" : { \"permission\" : { \"type\" : \"keyword\" }, \"user\" : { \"type\" : \"keyword\" } } } } }, \"tag\" : { \"properties\" : { \"creator\" : { \"type\" : \"keyword\" }, \"dateCreated\" : { \"type\" : \"date\" }, \"dateModified\" : { \"type\" : \"date\" }, \"description\" : { \"type\" : \"text\" }, \"id\" : { \"type\" : \"keyword\" }, \"targets\" : { \"type\" : \"nested\" , \"properties\" : { \"id\" : { \"type\" : \"keyword\" }, \"type\" : { \"type\" : \"keyword\" } } }, \"value\" : { \"type\" : \"text\" , \"analyzer\" : \"tag_value\" } } }, \"file_metadata\" : { \"_parent\" : { \"type\" : \"file\" }, \"_routing\" : { \"required\" : true }, \"properties\" : { \"id\" : { \"type\" : \"keyword\" }, \"metadata\" : { \"type\" : \"nested\" , \"properties\" : { \"attribute\" : { \"type\" : \"text\" , \"analyzer\" : \"irods_entity\" }, \"unit\" : { \"type\" : \"keyword\" }, \"value\" : { \"type\" : \"text\" } } } } }, \"folder\" : { \"properties\" : { \"creator\" : { \"type\" : \"keyword\" }, \"dateCreated\" : { \"type\" : \"date\" }, \"dateModified\" : { \"type\" : \"date\" }, \"fileSize\" : { \"type\" : \"long\" }, \"fileType\" : { \"type\" : \"keyword\" }, \"id\" : { \"type\" : \"keyword\" }, \"label\" : { \"type\" : \"text\" , \"fields\" : { \"keyword\" : { \"type\" : \"keyword\" } }, \"analyzer\" : \"irods_entity\" }, \"metadata\" : { \"type\" : \"nested\" , \"properties\" : { \"attribute\" : { \"type\" : \"text\" , \"analyzer\" : \"irods_entity\" }, \"unit\" : { \"type\" : \"keyword\" }, \"value\" : { \"type\" : \"text\" } } }, \"path\" : { \"type\" : \"text\" , \"fields\" : { \"keyword\" : { \"type\" : \"keyword\" } }, \"analyzer\" : \"irods_path\" }, \"userPermissions\" : { \"type\" : \"nested\" , \"properties\" : { \"permission\" : { \"type\" : \"keyword\" }, \"user\" : { \"type\" : \"keyword\" } } } } }, \"folder_metadata\" : { \"_parent\" : { \"type\" : \"folder\" }, \"_routing\" : { \"required\" : true }, \"properties\" : { \"id\" : { \"type\" : \"keyword\" }, \"metadata\" : { \"type\" : \"nested\" , \"properties\" : { \"attribute\" : { \"type\" : \"text\" , \"analyzer\" : \"irods_entity\" }, \"unit\" : { \"type\" : \"keyword\" }, \"value\" : { \"type\" : \"text\" } } } } } }, \"settings\" : { \"index\" : { \"mapper\" : { \"dynamic\" : \"false\" }, \"analysis\" : { \"analyzer\" : { \"irods_entity\" : { \"filter\" : [ \"asciifolding\" , \"lowercase\" ], \"type\" : \"custom\" , \"tokenizer\" : \"irods_entity\" }, \"irods_path\" : { \"type\" : \"custom\" , \"tokenizer\" : \"irods_path\" }, \"tag_value\" : { \"filter\" : [ \"asciifolding\" , \"lowercase\" ], \"type\" : \"custom\" , \"tokenizer\" : \"keyword\" } }, \"tokenizer\" : { \"irods_entity\" : { \"type\" : \"keyword\" , \"buffer_size\" : \"2700\" }, \"irods_path\" : { \"type\" : \"path_hierarchy\" , \"buffer_size\" : \"2700\" } } }, \"number_of_replicas\" : \"1\" } } } Index \u00b6 For indexing we will run a container inside your namespace where the elasticsearch is running, and copy the settings.json file inside this container and run the following commands: ## run container if es in prod namespace kubectl run --namespace = prod --rm utils -it --image arunvelsriram/utils bash ## run container if es in discover namespace # kubectl run --namespace=discover --rm utils -it --image arunvelsriram/utils bash ######### PS open a new terminal######### ## copy the settings.json if PROD kubectl -n prod cp settings.json utils:/home/utils ## copy the settings.json if Discover # kubectl -n discover cp settings.json utils:/home/utils run indexing \u00b6 Inside the running container shell run this command to add the indexes. # check elasticsearch health curl -XGET \"http://elasticsearch:9200/_cluster/health?pretty\" # run indexing from file curl -sX PUT \"http://elasticsearch:9200/data\" -d @settings.json # check indices of /data curl -XGET \"http://elasticsearch:9200/_cat/indices/data\" (optional) delete current indexes \u00b6 # delete data indexes curl -sX DELETE \"http://elasticsearch:9200/data\" # delete everything curl -sX DELETE \"http://elasticsearch:9200/*\" restart related services \u00b6 # kubectl rollout restart statefulset elasticsearch -n prod # not sure kubectl rollout restart deployment infosquito2 search -n <NAMESPACE>","title":"Elasticsearch"},{"location":"deployments/elasticsearch/#elasticsearch","text":"Currently deployed version: docker.elastic.co/elasticsearch/elasticsearch:5.6.14 Deploying the statefulsets Elasticsearch cluster on kubernetes.","title":"Elasticsearch"},{"location":"deployments/elasticsearch/#deploying","text":"","title":"Deploying"},{"location":"deployments/elasticsearch/#preq","text":"clone the repo see also k8s-resources cd /k8s-resources In case you have limited resources change these: vi resources/addons/elasticsearch/elasticsearch.yml replicas : 2 resources : requests : memory : \"4Gi\" limits : memory : \"4Gi\" - name : ES_JAVA_OPTS value : \"-Xms2g -Xmx2g\"","title":"Preq"},{"location":"deployments/elasticsearch/#deploy","text":"# deploy ES on prod env kubectl apply -n prod -f resources/addons/elasticsearch/elasticsearch.yml ## deploy ES on discover env # kubectl apply -n discover -f resources/addons/elasticsearch/elasticsearch.yml","title":"Deploy"},{"location":"deployments/elasticsearch/#indexing","text":"","title":"Indexing"},{"location":"deployments/elasticsearch/#preq_1","text":"Save below json to a file settings.json , we will use this file to index our elasticsearch. { \"mappings\" : { \"file\" : { \"properties\" : { \"creator\" : { \"type\" : \"keyword\" }, \"dateCreated\" : { \"type\" : \"date\" }, \"dateModified\" : { \"type\" : \"date\" }, \"fileSize\" : { \"type\" : \"long\" }, \"fileType\" : { \"type\" : \"keyword\" }, \"id\" : { \"type\" : \"keyword\" }, \"label\" : { \"type\" : \"text\" , \"fields\" : { \"keyword\" : { \"type\" : \"keyword\" } }, \"analyzer\" : \"irods_entity\" }, \"metadata\" : { \"type\" : \"nested\" , \"properties\" : { \"attribute\" : { \"type\" : \"text\" , \"analyzer\" : \"irods_entity\" }, \"unit\" : { \"type\" : \"keyword\" }, \"value\" : { \"type\" : \"text\" } } }, \"path\" : { \"type\" : \"text\" , \"fields\" : { \"keyword\" : { \"type\" : \"keyword\" } }, \"analyzer\" : \"irods_path\" }, \"userPermissions\" : { \"type\" : \"nested\" , \"properties\" : { \"permission\" : { \"type\" : \"keyword\" }, \"user\" : { \"type\" : \"keyword\" } } } } }, \"tag\" : { \"properties\" : { \"creator\" : { \"type\" : \"keyword\" }, \"dateCreated\" : { \"type\" : \"date\" }, \"dateModified\" : { \"type\" : \"date\" }, \"description\" : { \"type\" : \"text\" }, \"id\" : { \"type\" : \"keyword\" }, \"targets\" : { \"type\" : \"nested\" , \"properties\" : { \"id\" : { \"type\" : \"keyword\" }, \"type\" : { \"type\" : \"keyword\" } } }, \"value\" : { \"type\" : \"text\" , \"analyzer\" : \"tag_value\" } } }, \"file_metadata\" : { \"_parent\" : { \"type\" : \"file\" }, \"_routing\" : { \"required\" : true }, \"properties\" : { \"id\" : { \"type\" : \"keyword\" }, \"metadata\" : { \"type\" : \"nested\" , \"properties\" : { \"attribute\" : { \"type\" : \"text\" , \"analyzer\" : \"irods_entity\" }, \"unit\" : { \"type\" : \"keyword\" }, \"value\" : { \"type\" : \"text\" } } } } }, \"folder\" : { \"properties\" : { \"creator\" : { \"type\" : \"keyword\" }, \"dateCreated\" : { \"type\" : \"date\" }, \"dateModified\" : { \"type\" : \"date\" }, \"fileSize\" : { \"type\" : \"long\" }, \"fileType\" : { \"type\" : \"keyword\" }, \"id\" : { \"type\" : \"keyword\" }, \"label\" : { \"type\" : \"text\" , \"fields\" : { \"keyword\" : { \"type\" : \"keyword\" } }, \"analyzer\" : \"irods_entity\" }, \"metadata\" : { \"type\" : \"nested\" , \"properties\" : { \"attribute\" : { \"type\" : \"text\" , \"analyzer\" : \"irods_entity\" }, \"unit\" : { \"type\" : \"keyword\" }, \"value\" : { \"type\" : \"text\" } } }, \"path\" : { \"type\" : \"text\" , \"fields\" : { \"keyword\" : { \"type\" : \"keyword\" } }, \"analyzer\" : \"irods_path\" }, \"userPermissions\" : { \"type\" : \"nested\" , \"properties\" : { \"permission\" : { \"type\" : \"keyword\" }, \"user\" : { \"type\" : \"keyword\" } } } } }, \"folder_metadata\" : { \"_parent\" : { \"type\" : \"folder\" }, \"_routing\" : { \"required\" : true }, \"properties\" : { \"id\" : { \"type\" : \"keyword\" }, \"metadata\" : { \"type\" : \"nested\" , \"properties\" : { \"attribute\" : { \"type\" : \"text\" , \"analyzer\" : \"irods_entity\" }, \"unit\" : { \"type\" : \"keyword\" }, \"value\" : { \"type\" : \"text\" } } } } } }, \"settings\" : { \"index\" : { \"mapper\" : { \"dynamic\" : \"false\" }, \"analysis\" : { \"analyzer\" : { \"irods_entity\" : { \"filter\" : [ \"asciifolding\" , \"lowercase\" ], \"type\" : \"custom\" , \"tokenizer\" : \"irods_entity\" }, \"irods_path\" : { \"type\" : \"custom\" , \"tokenizer\" : \"irods_path\" }, \"tag_value\" : { \"filter\" : [ \"asciifolding\" , \"lowercase\" ], \"type\" : \"custom\" , \"tokenizer\" : \"keyword\" } }, \"tokenizer\" : { \"irods_entity\" : { \"type\" : \"keyword\" , \"buffer_size\" : \"2700\" }, \"irods_path\" : { \"type\" : \"path_hierarchy\" , \"buffer_size\" : \"2700\" } } }, \"number_of_replicas\" : \"1\" } } }","title":"Preq"},{"location":"deployments/elasticsearch/#index","text":"For indexing we will run a container inside your namespace where the elasticsearch is running, and copy the settings.json file inside this container and run the following commands: ## run container if es in prod namespace kubectl run --namespace = prod --rm utils -it --image arunvelsriram/utils bash ## run container if es in discover namespace # kubectl run --namespace=discover --rm utils -it --image arunvelsriram/utils bash ######### PS open a new terminal######### ## copy the settings.json if PROD kubectl -n prod cp settings.json utils:/home/utils ## copy the settings.json if Discover # kubectl -n discover cp settings.json utils:/home/utils","title":"Index"},{"location":"deployments/elasticsearch/#run-indexing","text":"Inside the running container shell run this command to add the indexes. # check elasticsearch health curl -XGET \"http://elasticsearch:9200/_cluster/health?pretty\" # run indexing from file curl -sX PUT \"http://elasticsearch:9200/data\" -d @settings.json # check indices of /data curl -XGET \"http://elasticsearch:9200/_cat/indices/data\"","title":"run indexing"},{"location":"deployments/elasticsearch/#optional-delete-current-indexes","text":"# delete data indexes curl -sX DELETE \"http://elasticsearch:9200/data\" # delete everything curl -sX DELETE \"http://elasticsearch:9200/*\"","title":"(optional) delete current indexes"},{"location":"deployments/elasticsearch/#restart-related-services","text":"# kubectl rollout restart statefulset elasticsearch -n prod # not sure kubectl rollout restart deployment infosquito2 search -n <NAMESPACE>","title":"restart related services"},{"location":"deployments/exim4/","text":"Mail \u00b6 exim4-helm A Helm chart to provide a exim4 deployment, exim4 (MTA) running as a smarthost. Deploy \u00b6 Add & update helm chart \u00b6 helm repo add exim4 https://mb-wali.github.io/exim4-helm helm repo update Install \u00b6 # replace the secrets with yours helm install exim4 --set secrets.EXIM_SMARTHOST = 'localhost' ,secrets.EXIM_PASSWORD = 'passw0rd' ,secrets.EXIM_ALLOWED_SENDERS = '*' exim4/exim4 --namespace mail --create-namespace --wait Debugging \u00b6 Once the pod is running # execute shell kubectl exec -it exim4-6ff546fb9f-ff47m -- bash # send a test mail echo \"This is test\" | mail -s \"The subject\" receiver@myhost.com -aFrom:sender@myhost.com Usage \u00b6 Use your deployed exim4 to send mails, e.g. connect from a another service. SMTP_HOST = exim4.mail.svc.cluster.local","title":"Mail"},{"location":"deployments/exim4/#mail","text":"exim4-helm A Helm chart to provide a exim4 deployment, exim4 (MTA) running as a smarthost.","title":"Mail"},{"location":"deployments/exim4/#deploy","text":"","title":"Deploy"},{"location":"deployments/exim4/#add-update-helm-chart","text":"helm repo add exim4 https://mb-wali.github.io/exim4-helm helm repo update","title":"Add &amp; update helm chart"},{"location":"deployments/exim4/#install","text":"# replace the secrets with yours helm install exim4 --set secrets.EXIM_SMARTHOST = 'localhost' ,secrets.EXIM_PASSWORD = 'passw0rd' ,secrets.EXIM_ALLOWED_SENDERS = '*' exim4/exim4 --namespace mail --create-namespace --wait","title":"Install"},{"location":"deployments/exim4/#debugging","text":"Once the pod is running # execute shell kubectl exec -it exim4-6ff546fb9f-ff47m -- bash # send a test mail echo \"This is test\" | mail -s \"The subject\" receiver@myhost.com -aFrom:sender@myhost.com","title":"Debugging"},{"location":"deployments/exim4/#usage","text":"Use your deployed exim4 to send mails, e.g. connect from a another service. SMTP_HOST = exim4.mail.svc.cluster.local","title":"Usage"},{"location":"deployments/grouper/","text":"Grouper \u00b6 Preq \u00b6 Make sure grouper database is setup, see also grouper-db Deploy \u00b6 Grouper consist of two deployments, grouper-loader & grouper-ws . grouper-loader \u00b6 ## Deploy for prod env kubectl apply -f resources/deployments/grouper-loader.yml -n prod ## Deploy for discover env # kubectl apply -f resources/deployments/grouper-loader.yml -n discover grouper-ws \u00b6 ## Deploy for prod env kubectl apply -f resources/deployments/grouper-ws.yml -n prod ## Deploy for discover env # kubectl apply -f resources/deployments/grouper-ws.yml -n discover","title":"Grouper"},{"location":"deployments/grouper/#grouper","text":"","title":"Grouper"},{"location":"deployments/grouper/#preq","text":"Make sure grouper database is setup, see also grouper-db","title":"Preq"},{"location":"deployments/grouper/#deploy","text":"Grouper consist of two deployments, grouper-loader & grouper-ws .","title":"Deploy"},{"location":"deployments/grouper/#grouper-loader","text":"## Deploy for prod env kubectl apply -f resources/deployments/grouper-loader.yml -n prod ## Deploy for discover env # kubectl apply -f resources/deployments/grouper-loader.yml -n discover","title":"grouper-loader"},{"location":"deployments/grouper/#grouper-ws","text":"## Deploy for prod env kubectl apply -f resources/deployments/grouper-ws.yml -n prod ## Deploy for discover env # kubectl apply -f resources/deployments/grouper-ws.yml -n discover","title":"grouper-ws"},{"location":"deployments/ingress-nginx/","text":"ingress-nginx \u00b6 ingress-nginx is used to give vice-apps ingresses, using nodeport and what not. deploy \u00b6 The deployment manifests are in k8s-resources . (optional) changing the env \u00b6 Modify file resources/kustomize/ingress-nginx/overlays/prod/args.yaml , to change the namespace. - --default-backend-service=prod/vice-default-backend + --default-backend-service=discover/vice-default-backend Deploy kustomize \u00b6 This script will create a namespace ingress-nginx . kubectl apply -k resources/kustomize/ingress-nginx/overlays/prod Issues \u00b6 failed calling webhook \"Internal error occurred: failed calling webhook \\\"validate.nginx.ingress.kubernetes.io\\\": Post \\\"https://ingress-nginx-controller-admission.ingress-nginx.svc:443/networking/v1/ingresses?timeout=10s\\\": service \\\"ingress-nginx-controller-admission\\\" not found\" solution kubectl delete -A ValidatingWebhookConfiguration ingress-nginx-admission","title":"ingress-nginx"},{"location":"deployments/ingress-nginx/#ingress-nginx","text":"ingress-nginx is used to give vice-apps ingresses, using nodeport and what not.","title":"ingress-nginx"},{"location":"deployments/ingress-nginx/#deploy","text":"The deployment manifests are in k8s-resources .","title":"deploy"},{"location":"deployments/ingress-nginx/#optional-changing-the-env","text":"Modify file resources/kustomize/ingress-nginx/overlays/prod/args.yaml , to change the namespace. - --default-backend-service=prod/vice-default-backend + --default-backend-service=discover/vice-default-backend","title":"(optional) changing the env"},{"location":"deployments/ingress-nginx/#deploy-kustomize","text":"This script will create a namespace ingress-nginx . kubectl apply -k resources/kustomize/ingress-nginx/overlays/prod","title":"Deploy kustomize"},{"location":"deployments/ingress-nginx/#issues","text":"failed calling webhook \"Internal error occurred: failed calling webhook \\\"validate.nginx.ingress.kubernetes.io\\\": Post \\\"https://ingress-nginx-controller-admission.ingress-nginx.svc:443/networking/v1/ingresses?timeout=10s\\\": service \\\"ingress-nginx-controller-admission\\\" not found\" solution kubectl delete -A ValidatingWebhookConfiguration ingress-nginx-admission","title":"Issues"},{"location":"deployments/irods-csi-driver/","text":"iRODS CSI Driver \u00b6 Currently deployed version: 0.11.0 iRODS Container Storage Interface (CSI) Driver implements the CSI Specification to provide container orchestration engines (like Kubernetes) iRODS access. Preq \u00b6 Before we install irods-csi-driver , we need to create a values.yaml file. Change the values accordingly and save the file as values.yaml . globalConfig : secret : stringData : client : \"irodsfuse\" host : <IRODS-SERVER-HOST> port : \"1247\" zone : \"ZONE\" user : <IRODS-ADMIN-USER> password : <PASSWORD> retainData : \"false\" enforceProxyAccess : \"true\" mountPathWhitelist : \"/ZONE/home\" nodeService : irodsPool : extraArgs : - --cache_size_max=10737418240 - '--cache_timeout_settings=[{\"path\":\"/\",\"timeout\":\"-1ns\",\"inherit\":false},{\"path\":\"/ZONE\",\"timeout\":\"-1ns\",\"inherit\":false},{\"path\":\"/ZONE/home\",\"timeout\":\"1h\",\"inherit\":false},{\"path\":\"/ZONE/home/shared\",\"timeout\":\"1h\",\"inherit\":true}]' - --data_root=/irodsfs-pool Deploy \u00b6 we use helm to deploy irods-csi-driver. # Add the Helm repository. helm repo add irods-csi-driver-repo https://cyverse.github.io/irods-csi-driver-helm/ # Update the local repository caches. helm repo update # create namespace kubectl create namespace irods-csi-driver # install csi-driver helm install -n irods-csi-driver irods-csi-driver irods-csi-driver-repo/irods-csi-driver -f ./values.yaml # or upgrade helm upgrade -n irods-csi-driver irods-csi-driver irods-csi-driver-repo/irods-csi-driver -f ./values.yaml Upgrading to a newer version \u00b6 When we want to upgrade the irods-csi-driver to a newer version, we need to stop all running vice-apps and delete all the pvcs. # update helm repo helm repo update # delete all the vice-apps deployments ## see below for the content of this file ./nuke-vice-analysis.sh $( kubectl get deployments -n vice-apps -l app-type = interactive -o name ) # delete the pvc kubectl delete pvc -l app-type = interactive -n vice-apps # uninstall the irods-csi-driver helm uninstall irods-csi-driver -n irods-csi-driver # install again helm install -n irods-csi-driver irods-csi-driver irods-csi-driver-repo/irods-csi-driver -f values.yaml install specific version \u00b6 helm install -n irods-csi-driver irods-csi-driver --version 0 .9.9 irods-csi-driver-repo/irods-csi-driver -f values.yaml NOTE \u00b6 With irods-csi-driver version <= 0.8.7 , the extraArgs of values.yaml file will look like: extraArgs : - --cache_size_max=10737418240 - --cache_root=/irodsfs_pool_cache - '--cache_timeout_settings=[{\"path\":\"/\",\"timeout\":\"-1ns\",\"inherit\":false},{\"path\":\"/ZONE\",\"timeout\":\"-1ns\",\"inherit\":false},{\"path\":\"/ZONE/home\",\"timeout\":\"1h\",\"inherit\":false},{\"path\":\"/ZONE/home/shared\",\"timeout\":\"1h\",\"inherit\":true}]' nuke-vice-analysis.sh \u00b6 function delete_resources () { local external_id = \" $1 \" kubectl -n vice-apps delete deployment \" ${ external_id } \" kubectl -n vice-apps delete service \"vice- ${ external_id } \" kubectl -n vice-apps delete ingress \" ${ external_id } \" kubectl -n vice-apps delete configmap \"excludes-file- ${ external_id } \" kubectl -n vice-apps delete configmap \"input-path-list- ${ external_id } \" } function remove_deployment_prefix () { local external_id = \" $1 \" echo -n \" $external_id \" | sed 's;^deployment.apps/;;' } # Iterate over all arguments on the command line. for id in \" $@ \" ; do delete_resources $( remove_deployment_prefix \" $id \" ) done","title":"Irods CSI Driver"},{"location":"deployments/irods-csi-driver/#irods-csi-driver","text":"Currently deployed version: 0.11.0 iRODS Container Storage Interface (CSI) Driver implements the CSI Specification to provide container orchestration engines (like Kubernetes) iRODS access.","title":"iRODS CSI Driver"},{"location":"deployments/irods-csi-driver/#preq","text":"Before we install irods-csi-driver , we need to create a values.yaml file. Change the values accordingly and save the file as values.yaml . globalConfig : secret : stringData : client : \"irodsfuse\" host : <IRODS-SERVER-HOST> port : \"1247\" zone : \"ZONE\" user : <IRODS-ADMIN-USER> password : <PASSWORD> retainData : \"false\" enforceProxyAccess : \"true\" mountPathWhitelist : \"/ZONE/home\" nodeService : irodsPool : extraArgs : - --cache_size_max=10737418240 - '--cache_timeout_settings=[{\"path\":\"/\",\"timeout\":\"-1ns\",\"inherit\":false},{\"path\":\"/ZONE\",\"timeout\":\"-1ns\",\"inherit\":false},{\"path\":\"/ZONE/home\",\"timeout\":\"1h\",\"inherit\":false},{\"path\":\"/ZONE/home/shared\",\"timeout\":\"1h\",\"inherit\":true}]' - --data_root=/irodsfs-pool","title":"Preq"},{"location":"deployments/irods-csi-driver/#deploy","text":"we use helm to deploy irods-csi-driver. # Add the Helm repository. helm repo add irods-csi-driver-repo https://cyverse.github.io/irods-csi-driver-helm/ # Update the local repository caches. helm repo update # create namespace kubectl create namespace irods-csi-driver # install csi-driver helm install -n irods-csi-driver irods-csi-driver irods-csi-driver-repo/irods-csi-driver -f ./values.yaml # or upgrade helm upgrade -n irods-csi-driver irods-csi-driver irods-csi-driver-repo/irods-csi-driver -f ./values.yaml","title":"Deploy"},{"location":"deployments/irods-csi-driver/#upgrading-to-a-newer-version","text":"When we want to upgrade the irods-csi-driver to a newer version, we need to stop all running vice-apps and delete all the pvcs. # update helm repo helm repo update # delete all the vice-apps deployments ## see below for the content of this file ./nuke-vice-analysis.sh $( kubectl get deployments -n vice-apps -l app-type = interactive -o name ) # delete the pvc kubectl delete pvc -l app-type = interactive -n vice-apps # uninstall the irods-csi-driver helm uninstall irods-csi-driver -n irods-csi-driver # install again helm install -n irods-csi-driver irods-csi-driver irods-csi-driver-repo/irods-csi-driver -f values.yaml","title":"Upgrading to a newer version"},{"location":"deployments/irods-csi-driver/#install-specific-version","text":"helm install -n irods-csi-driver irods-csi-driver --version 0 .9.9 irods-csi-driver-repo/irods-csi-driver -f values.yaml","title":"install specific version"},{"location":"deployments/irods-csi-driver/#note","text":"With irods-csi-driver version <= 0.8.7 , the extraArgs of values.yaml file will look like: extraArgs : - --cache_size_max=10737418240 - --cache_root=/irodsfs_pool_cache - '--cache_timeout_settings=[{\"path\":\"/\",\"timeout\":\"-1ns\",\"inherit\":false},{\"path\":\"/ZONE\",\"timeout\":\"-1ns\",\"inherit\":false},{\"path\":\"/ZONE/home\",\"timeout\":\"1h\",\"inherit\":false},{\"path\":\"/ZONE/home/shared\",\"timeout\":\"1h\",\"inherit\":true}]'","title":"NOTE"},{"location":"deployments/irods-csi-driver/#nuke-vice-analysissh","text":"function delete_resources () { local external_id = \" $1 \" kubectl -n vice-apps delete deployment \" ${ external_id } \" kubectl -n vice-apps delete service \"vice- ${ external_id } \" kubectl -n vice-apps delete ingress \" ${ external_id } \" kubectl -n vice-apps delete configmap \"excludes-file- ${ external_id } \" kubectl -n vice-apps delete configmap \"input-path-list- ${ external_id } \" } function remove_deployment_prefix () { local external_id = \" $1 \" echo -n \" $external_id \" | sed 's;^deployment.apps/;;' } # Iterate over all arguments on the command line. for id in \" $@ \" ; do delete_resources $( remove_deployment_prefix \" $id \" ) done","title":"nuke-vice-analysis.sh"},{"location":"deployments/jaeger/","text":"jaeger \u00b6 Jaeger: open source, end-to-end distributed tracing Deploy \u00b6 The deployment manifests are in k8s-resources . Create namespace kubectl create ns jaeger (optional) changing the env Modify files to change the namespace: resources/addons/jaeger/collector.yaml resources/addons/jaeger/query.yaml resources/addons/jaeger/rollover-cron.yaml - \"http://elasticsearch.prod:9200\" + \"http://elasticsearch.discover:9200\" Apply manifests kubectl apply -f resources/addons/jaeger/rollover-cron.yaml -n jaeger kubectl apply -f resources/addons/jaeger/query.yaml -n jaeger kubectl apply -f resources/addons/jaeger/collector.yaml -n jaeger","title":"Jaeger"},{"location":"deployments/jaeger/#jaeger","text":"Jaeger: open source, end-to-end distributed tracing","title":"jaeger"},{"location":"deployments/jaeger/#deploy","text":"The deployment manifests are in k8s-resources . Create namespace kubectl create ns jaeger (optional) changing the env Modify files to change the namespace: resources/addons/jaeger/collector.yaml resources/addons/jaeger/query.yaml resources/addons/jaeger/rollover-cron.yaml - \"http://elasticsearch.prod:9200\" + \"http://elasticsearch.discover:9200\" Apply manifests kubectl apply -f resources/addons/jaeger/rollover-cron.yaml -n jaeger kubectl apply -f resources/addons/jaeger/query.yaml -n jaeger kubectl apply -f resources/addons/jaeger/collector.yaml -n jaeger","title":"Deploy"},{"location":"deployments/k8s-resources/","text":"k8s-resources \u00b6 This repository includes all the manifests and resources for kubernetes deployment. Clone \u00b6 git clone git@gitlab.cyverse.org:tugraz/k8s-resources.git Generate config/secrets for prod env \u00b6 Make sure gomplate is installed in your OS. Here is an example on ubuntu: sudo curl -o /usr/local/bin/gomplate -sSL https://github.com/hairyhenderson/gomplate/releases/download/v3.11.5/gomplate_linux-amd64 sudo chmod 755 /usr/local/bin/gomplate gomplate --help Generate configs/secrets ./generate_configs.py -e prod ./generate_secrets.py -e prod Load config/secrets in your cluster for prod env \u00b6 ./load_configs.py -e prod -n prod ./load_secrets.py -e prod -n prod Deploy services \u00b6 preq \u00b6 Before we start to deploy the services we need to create these secrets, and install some dependencies. vice-image-pull-secret TODO: kubectl apply. harbor-registry-credentials TODO: kubectl apply. These secrets below can be deployed using: git clone -b discover https://gitlab.cyverse.org/tugraz/docker-tugraz-data see also cyverse.at repo. git clone -b discover git@gitlab.cyverse.org:mbwali/docker-tugraz-data.git cd docker-tugraz-data # run for discover env go run loadsecrets.go --env discover --namespace discover --envtype de This will create the followings: gpg-keys ui-nginx-tls pgpass-files signing-keys accepted-keys ssl-files Make sure elasticsearch is deployed. Make sure skaffold is installed in your OS. # For Linux x86_64 (amd64) curl -Lo skaffold https://storage.googleapis.com/skaffold/releases/latest/skaffold-linux-amd64 sudo install skaffold /usr/local/bin/ # check skaffold --help Make sure ServiceAccounts are created: # For prod env kubectl apply -f /k8s-resources/resources/serviceaccounts/app-exposer.yml -n prod ## For discover env # kubectl apply -f /k8s-resources/resources/serviceaccounts/app-exposer.yml -n discover # configurator kubectl create serviceaccount configurator -n discover TODO above \u00b6 Deploy \u00b6 This command will deploy all the services listed on k8s-resources/repos # deploy services for prod env ./deploy.py -n prod -BCa ## deploy services for discover env # ./deploy.py -n discover -BCa Deploy single service \u00b6 If you want to deploy a single service, e.g. search ## Run for the prod env ./deploy.py -Bn prod -p search -C ## Run for the discover env # ./deploy.py -Bn discover -p search -C create a new env \u00b6 Currently we have prod environment which is our productive instance. Create your environment: cp config_values/prod.yaml config_values/discover.yaml This will create a new config file for your environment. You will have to update all the values as you see fit for your environment. fill in all these values: # discover.yaml --- Environment : Agave : Key : Secret : RedirectURI : StorageSystem : CallbackBaseURI : ReadTimeout : Enabled : JobsEnabled : AMQP : URI : AnonFiles : BaseURI : AppExposer : BaseURI : BaseURLs : Analyses : Apps : AsyncTasks : DashboardAggregator : DataInfo : GrouperWebServices : IplantEmail : IplantGroups : JexAdapter : Metadata : Notifications : Permissions : Requests : Search : Terrain : UserInfo : CAS : BaseURI : ServerName : UIDDomain : DashboardAggregator : PublicGroup : LogLevel : DataOne : BaseURI : DE : Version : VersionName : AMQP : URI : Host : BaseURI : Legacy : BaseURI : Subscriptions : CheckoutURL : KeepAlive : Service : Target : ContextMenu : Enabled : BaseTrash : Path : ProdDeployment : DefaultOutputFolder : WSO2 : JWTHeader : Coge : BaseURI : Tools : Admin : MaxCpuLimit : MaxMemoryLimit : MaxDiskLimit : Docker : TrustedRegistries : Tag : Elasticsearch : BaseURI : Username : Password : Index : Email : AppDeletion : Src : Dest : AppPublicationRequest : Src : Dest : ToolRequest : Src : Dest : PermIDRequest : Src : Dest : Support : Src : Dest : Grouper : Environment : MorphString : WebService : Password : Password : DB : User : Password : Host : Port : Name : FolderNamePrefix : Loader : URI : User : Password : SubjectSource : ID : Name : SearchBase : ICAT : Host : Port : User : Password : Infosquito : DayNum : PrefixLength : InteractiveApps : BaseURI : ServiceSuffix : Intercom : AppID : CompanyID : CompanyName : Intercom : IRODS : AMQP : URI : Host : User : Zone : Password : AdminUsers : PermsFilter : ExternalHost : QuotaRootResources : Jobs : DataTransferImage : JobStatusListener : BaseURI : Keycloak : ServerURI : Realm : ClientID : ClientSecret : VICE : ClientID : ClientSecret : Kifshare : ExternalUri : PGP : KeyPassword : PermanentID : CuratorsGroup : DataCite : BaseURI : User : Password : DOIPrefix : Redis : Host : Port : HA : Name : Password : DB : Number : TimeZone : Vault : Token : URL : IRODS : MountPath : ChildToken : UseLimit : VICE : DB : User : Password : Host : Port : Name : FileTransfers : Image : Tag : JobStatus : BaseURI : K8sEnabled : BackendNamespace : ImagePullSecret : ImageCache : UseCSIDriver : DefaultImage : DefaultName : DefaultCasUrl : DefaultCasValidate : ConcurrentJobs : UseCaseCharsMin : DefaultBackend : LoadingPageTemplateString : Sonora : BaseURI : Terrain : CASClientID : CASClientSecret : JWT : SigningKey : Password : Unleash : BaseUrl : APIPath : APIToken : MaintenanceFlag : UserPortal : BaseURI : DEDB : User : Password : Host : Port : Name : NewNotificationsDB : User : Password : Host : Port : Name : NotificationsDB : User : Password : Host : Port : Name : PermissionsDB : User : Password : Host : Port : Name : QMSDB : User : Password : Host : Port : Name : Reinitialize : MetadataDB : User : Password : Host : Port : Name : UnleashDB : User : Password : Host : Port : Name : Admin : Groups : Attribute : FileIdentifier : HtPathList : MultiInputPathList : Analytics : Enabled : Id : Harbor : URL : ProjectQARobotName : ProjectQARobotSecret : QMS : Enabled : Base : Usage : Jaeger : Endpoint : Generate config/secrets for discover env \u00b6 ./generate_configs.py -e discover ./generate_secrets.py -e discover Load config/secrets in your cluster for discover env \u00b6 ./load_configs.py -e discover -n discover ./load_secrets.py -e discover -n discover","title":"K8s-resources"},{"location":"deployments/k8s-resources/#k8s-resources","text":"This repository includes all the manifests and resources for kubernetes deployment.","title":"k8s-resources"},{"location":"deployments/k8s-resources/#clone","text":"git clone git@gitlab.cyverse.org:tugraz/k8s-resources.git","title":"Clone"},{"location":"deployments/k8s-resources/#generate-configsecrets-for-prod-env","text":"Make sure gomplate is installed in your OS. Here is an example on ubuntu: sudo curl -o /usr/local/bin/gomplate -sSL https://github.com/hairyhenderson/gomplate/releases/download/v3.11.5/gomplate_linux-amd64 sudo chmod 755 /usr/local/bin/gomplate gomplate --help Generate configs/secrets ./generate_configs.py -e prod ./generate_secrets.py -e prod","title":"Generate config/secrets for prod env"},{"location":"deployments/k8s-resources/#load-configsecrets-in-your-cluster-for-prod-env","text":"./load_configs.py -e prod -n prod ./load_secrets.py -e prod -n prod","title":"Load config/secrets in your cluster for prod env"},{"location":"deployments/k8s-resources/#deploy-services","text":"","title":"Deploy services"},{"location":"deployments/k8s-resources/#preq","text":"Before we start to deploy the services we need to create these secrets, and install some dependencies. vice-image-pull-secret TODO: kubectl apply. harbor-registry-credentials TODO: kubectl apply. These secrets below can be deployed using: git clone -b discover https://gitlab.cyverse.org/tugraz/docker-tugraz-data see also cyverse.at repo. git clone -b discover git@gitlab.cyverse.org:mbwali/docker-tugraz-data.git cd docker-tugraz-data # run for discover env go run loadsecrets.go --env discover --namespace discover --envtype de This will create the followings: gpg-keys ui-nginx-tls pgpass-files signing-keys accepted-keys ssl-files Make sure elasticsearch is deployed. Make sure skaffold is installed in your OS. # For Linux x86_64 (amd64) curl -Lo skaffold https://storage.googleapis.com/skaffold/releases/latest/skaffold-linux-amd64 sudo install skaffold /usr/local/bin/ # check skaffold --help Make sure ServiceAccounts are created: # For prod env kubectl apply -f /k8s-resources/resources/serviceaccounts/app-exposer.yml -n prod ## For discover env # kubectl apply -f /k8s-resources/resources/serviceaccounts/app-exposer.yml -n discover # configurator kubectl create serviceaccount configurator -n discover","title":"preq"},{"location":"deployments/k8s-resources/#todo-above","text":"","title":"TODO above"},{"location":"deployments/k8s-resources/#deploy","text":"This command will deploy all the services listed on k8s-resources/repos # deploy services for prod env ./deploy.py -n prod -BCa ## deploy services for discover env # ./deploy.py -n discover -BCa","title":"Deploy"},{"location":"deployments/k8s-resources/#deploy-single-service","text":"If you want to deploy a single service, e.g. search ## Run for the prod env ./deploy.py -Bn prod -p search -C ## Run for the discover env # ./deploy.py -Bn discover -p search -C","title":"Deploy single service"},{"location":"deployments/k8s-resources/#create-a-new-env","text":"Currently we have prod environment which is our productive instance. Create your environment: cp config_values/prod.yaml config_values/discover.yaml This will create a new config file for your environment. You will have to update all the values as you see fit for your environment. fill in all these values: # discover.yaml --- Environment : Agave : Key : Secret : RedirectURI : StorageSystem : CallbackBaseURI : ReadTimeout : Enabled : JobsEnabled : AMQP : URI : AnonFiles : BaseURI : AppExposer : BaseURI : BaseURLs : Analyses : Apps : AsyncTasks : DashboardAggregator : DataInfo : GrouperWebServices : IplantEmail : IplantGroups : JexAdapter : Metadata : Notifications : Permissions : Requests : Search : Terrain : UserInfo : CAS : BaseURI : ServerName : UIDDomain : DashboardAggregator : PublicGroup : LogLevel : DataOne : BaseURI : DE : Version : VersionName : AMQP : URI : Host : BaseURI : Legacy : BaseURI : Subscriptions : CheckoutURL : KeepAlive : Service : Target : ContextMenu : Enabled : BaseTrash : Path : ProdDeployment : DefaultOutputFolder : WSO2 : JWTHeader : Coge : BaseURI : Tools : Admin : MaxCpuLimit : MaxMemoryLimit : MaxDiskLimit : Docker : TrustedRegistries : Tag : Elasticsearch : BaseURI : Username : Password : Index : Email : AppDeletion : Src : Dest : AppPublicationRequest : Src : Dest : ToolRequest : Src : Dest : PermIDRequest : Src : Dest : Support : Src : Dest : Grouper : Environment : MorphString : WebService : Password : Password : DB : User : Password : Host : Port : Name : FolderNamePrefix : Loader : URI : User : Password : SubjectSource : ID : Name : SearchBase : ICAT : Host : Port : User : Password : Infosquito : DayNum : PrefixLength : InteractiveApps : BaseURI : ServiceSuffix : Intercom : AppID : CompanyID : CompanyName : Intercom : IRODS : AMQP : URI : Host : User : Zone : Password : AdminUsers : PermsFilter : ExternalHost : QuotaRootResources : Jobs : DataTransferImage : JobStatusListener : BaseURI : Keycloak : ServerURI : Realm : ClientID : ClientSecret : VICE : ClientID : ClientSecret : Kifshare : ExternalUri : PGP : KeyPassword : PermanentID : CuratorsGroup : DataCite : BaseURI : User : Password : DOIPrefix : Redis : Host : Port : HA : Name : Password : DB : Number : TimeZone : Vault : Token : URL : IRODS : MountPath : ChildToken : UseLimit : VICE : DB : User : Password : Host : Port : Name : FileTransfers : Image : Tag : JobStatus : BaseURI : K8sEnabled : BackendNamespace : ImagePullSecret : ImageCache : UseCSIDriver : DefaultImage : DefaultName : DefaultCasUrl : DefaultCasValidate : ConcurrentJobs : UseCaseCharsMin : DefaultBackend : LoadingPageTemplateString : Sonora : BaseURI : Terrain : CASClientID : CASClientSecret : JWT : SigningKey : Password : Unleash : BaseUrl : APIPath : APIToken : MaintenanceFlag : UserPortal : BaseURI : DEDB : User : Password : Host : Port : Name : NewNotificationsDB : User : Password : Host : Port : Name : NotificationsDB : User : Password : Host : Port : Name : PermissionsDB : User : Password : Host : Port : Name : QMSDB : User : Password : Host : Port : Name : Reinitialize : MetadataDB : User : Password : Host : Port : Name : UnleashDB : User : Password : Host : Port : Name : Admin : Groups : Attribute : FileIdentifier : HtPathList : MultiInputPathList : Analytics : Enabled : Id : Harbor : URL : ProjectQARobotName : ProjectQARobotSecret : QMS : Enabled : Base : Usage : Jaeger : Endpoint :","title":"create a new env"},{"location":"deployments/k8s-resources/#generate-configsecrets-for-discover-env","text":"./generate_configs.py -e discover ./generate_secrets.py -e discover","title":"Generate config/secrets for discover env"},{"location":"deployments/k8s-resources/#load-configsecrets-in-your-cluster-for-discover-env","text":"./load_configs.py -e discover -n discover ./load_secrets.py -e discover -n discover","title":"Load config/secrets in your cluster for discover env"},{"location":"deployments/keycloak/","text":"Keycloak \u00b6 Current keycloak version: 22.0 Preq \u00b6 configure database \u00b6 To setup and configure database please have a look at keycloak database . create namsespace \u00b6 # create ns kubectl create ns keycloak Create required secrets & configmap \u00b6 Keycloak deployment requires secrets and configmaps, which can be done via a kustomization.yaml file, please see below for an example of this template: secretGenerator : - name : dbuser # Database literals : - username=<USERNAME> - password=<PASSWORD> - name : kcadmin # keyclaok literals : - username=<USERNAME> - password=<PASSWORD> configMapGenerator : - name : keycloak-config literals : - KEYCLOAK_HOSTNAME=https://keycloak.example.com/auth - KEYCLOAK_HOSTNAME_STRICT_HTTPS=false - KEYCLOAK_HOSTNAME_STRICT=false - KEYCLOAK_LOGLEVEL=INFO - KEYCLOAK_PROXY=edge - DB_VENDOR=postgres - DB_ADDR=<DATABASE_HOST> - DB_PORT=5432 - PROXY_ADDRESS_FORWARDING=true - JDBC_PARAMS=connectTimeout=21600 - JAVA_OPTS=-server -Xms4096m -Xmx8192m -XX:MetaspaceSize=96m -XX:MaxMetaspaceSize=256m -Djboss.modules.system.pkgs=org.jboss.byteman -Djava.awt.headless=true -Dkeycloak.profile.feature.token_exchange=enabled -Djava.security.egd=file:/dev/urandom namespace : keycloak resources : - deployment.yaml - service.yaml generatorOptions : disableNameSuffixHash : true Update the values of the kustomization.yaml file. place the kustomization.yaml , deployment.yaml and service.yaml inside a directory. e.g. base deployment & service YAML files: \u00b6 TODO: find a way to add those files. Deploy \u00b6 # apply kustomize kubectl apply -k ./base/ -n keycloak \u00b6 Keycloak: Setting Up a New Realm for CyVerse \u00b6 This guide walks you through the process of creating and configuring a fresh Keycloak realm for CyVerse . 1. Create the Realm \u00b6 Log in to the Keycloak admin console. Create a new realm named: CyVerse 2. Import Clients \u00b6 Import predefined clients from the following repository: \ud83d\udcc2 Clients Configuration Files Note: These configuration files are currently private. TODO: Make the repository public for easier access. 3. Add Mappers and Groups \u00b6 Set up necessary mappers and user groups as outlined in the repository: \ud83d\udcd8 Admin Configuration Guide TODO: Ensure this documentation is made public if sharing with external teams.","title":"Keycloak"},{"location":"deployments/keycloak/#keycloak","text":"Current keycloak version: 22.0","title":"Keycloak"},{"location":"deployments/keycloak/#preq","text":"","title":"Preq"},{"location":"deployments/keycloak/#configure-database","text":"To setup and configure database please have a look at keycloak database .","title":"configure database"},{"location":"deployments/keycloak/#create-namsespace","text":"# create ns kubectl create ns keycloak","title":"create namsespace"},{"location":"deployments/keycloak/#create-required-secrets-configmap","text":"Keycloak deployment requires secrets and configmaps, which can be done via a kustomization.yaml file, please see below for an example of this template: secretGenerator : - name : dbuser # Database literals : - username=<USERNAME> - password=<PASSWORD> - name : kcadmin # keyclaok literals : - username=<USERNAME> - password=<PASSWORD> configMapGenerator : - name : keycloak-config literals : - KEYCLOAK_HOSTNAME=https://keycloak.example.com/auth - KEYCLOAK_HOSTNAME_STRICT_HTTPS=false - KEYCLOAK_HOSTNAME_STRICT=false - KEYCLOAK_LOGLEVEL=INFO - KEYCLOAK_PROXY=edge - DB_VENDOR=postgres - DB_ADDR=<DATABASE_HOST> - DB_PORT=5432 - PROXY_ADDRESS_FORWARDING=true - JDBC_PARAMS=connectTimeout=21600 - JAVA_OPTS=-server -Xms4096m -Xmx8192m -XX:MetaspaceSize=96m -XX:MaxMetaspaceSize=256m -Djboss.modules.system.pkgs=org.jboss.byteman -Djava.awt.headless=true -Dkeycloak.profile.feature.token_exchange=enabled -Djava.security.egd=file:/dev/urandom namespace : keycloak resources : - deployment.yaml - service.yaml generatorOptions : disableNameSuffixHash : true Update the values of the kustomization.yaml file. place the kustomization.yaml , deployment.yaml and service.yaml inside a directory. e.g. base","title":"Create required secrets &amp; configmap"},{"location":"deployments/keycloak/#deployment-service-yaml-files","text":"TODO: find a way to add those files.","title":"deployment &amp; service YAML files:"},{"location":"deployments/keycloak/#deploy","text":"","title":"Deploy"},{"location":"deployments/keycloak/#apply-kustomize-kubectl-apply-k-base-n-keycloak","text":"","title":"# apply kustomize kubectl apply -k ./base/ -n keycloak"},{"location":"deployments/keycloak/#keycloak-setting-up-a-new-realm-for-cyverse","text":"This guide walks you through the process of creating and configuring a fresh Keycloak realm for CyVerse .","title":"Keycloak: Setting Up a New Realm for CyVerse"},{"location":"deployments/keycloak/#1-create-the-realm","text":"Log in to the Keycloak admin console. Create a new realm named: CyVerse","title":"1. Create the Realm"},{"location":"deployments/keycloak/#2-import-clients","text":"Import predefined clients from the following repository: \ud83d\udcc2 Clients Configuration Files Note: These configuration files are currently private. TODO: Make the repository public for easier access.","title":"2. Import Clients"},{"location":"deployments/keycloak/#3-add-mappers-and-groups","text":"Set up necessary mappers and user groups as outlined in the repository: \ud83d\udcd8 Admin Configuration Guide TODO: Ensure this documentation is made public if sharing with external teams.","title":"3. Add Mappers and Groups"},{"location":"deployments/kubernetes-deploy/","text":"Kubernetes Cluster \u00b6 You can use Ansible to set up a kubernetes cluster. Current kubernetes cluster is configured using ansible playbooks from https://github.com/cyverse-austria/ansible-k8s.git . You can use this ansible playbook to setup your own kubernetes Cluster on Operating Systems such as: Centos7, Rocky Linux 8 and Debian 11 . Looking to setup your own k8s cluster? \u00b6 If your are intrested to setup your own kubernetes cluster please follow the steps bellow. Preq \u00b6 Make sure you have at least 6 virtual machines configured with Centos7 \u00b6 1 Master node 4 worker nodes 1 worker node dedicated only for vice-apps Make sure you have Ansible installed, and you can reach your virtual machines via ssh \u00b6 Steps \u00b6 Clone the repo \u00b6 # clone repo git clone https://github.com/cyverse-austria/ansible-k8s.git # navigate to cloned repo cd ansible-k8s create your inventory \u00b6 Create your inventory file under /inventory/MYINVENTORY and replace the host names as yours. [ k8s:children ] k8s-control-plane k8s-worker [ kube-apiserver-haproxy ] k8-haproxy [ k8s-control-plane ] k8-c01 k8-c02 [ k8s-storage:children ] k8s-worker [ k8s-worker:children ] vice-workers k8s-workers [ k8s-workers ] k8-w01 k8-w02 [ vice-workers ] k8-vice-w01 [ outward-facing-proxy ] vice-haproxy-01 [ haproxy ] [ loadbalancer ] haproxy-01 Run playbooks \u00b6 Note: we are using --user root --become , if your virtual machines have a diffrent user you could change this. Check if your hosts are reachable \u00b6 ansible -i /inventory/MYINVENTORY -m ping all --user root --become Setup firewall configs \u00b6 ansible-playbook -i /inventory/MYINVENTORY firewalld-config.yml --user root --become Provision your nodes \u00b6 ansible-playbook -i /inventory/MYINVENTORY provision-nodes.yml --user root --become Init master/worker nodes \u00b6 ansible-playbook -i /inventory/MYINVENTORY multi-master.yml --user root --become Tainting and Labeling VICE Worker Nodes \u00b6 kubectl label nodes k8-vice-w01 vice = true kubectl taint nodes k8-vice-w01 vice = only:NoSchedule","title":"Kubernetes"},{"location":"deployments/kubernetes-deploy/#kubernetes-cluster","text":"You can use Ansible to set up a kubernetes cluster. Current kubernetes cluster is configured using ansible playbooks from https://github.com/cyverse-austria/ansible-k8s.git . You can use this ansible playbook to setup your own kubernetes Cluster on Operating Systems such as: Centos7, Rocky Linux 8 and Debian 11 .","title":"Kubernetes Cluster"},{"location":"deployments/kubernetes-deploy/#looking-to-setup-your-own-k8s-cluster","text":"If your are intrested to setup your own kubernetes cluster please follow the steps bellow.","title":"Looking to setup your own k8s cluster?"},{"location":"deployments/kubernetes-deploy/#preq","text":"","title":"Preq"},{"location":"deployments/kubernetes-deploy/#make-sure-you-have-at-least-6-virtual-machines-configured-with-centos7","text":"1 Master node 4 worker nodes 1 worker node dedicated only for vice-apps","title":"Make sure you have at least 6 virtual machines configured with Centos7"},{"location":"deployments/kubernetes-deploy/#make-sure-you-have-ansible-installed-and-you-can-reach-your-virtual-machines-via-ssh","text":"","title":"Make sure you have Ansible installed, and you can reach your virtual machines via ssh"},{"location":"deployments/kubernetes-deploy/#steps","text":"","title":"Steps"},{"location":"deployments/kubernetes-deploy/#clone-the-repo","text":"# clone repo git clone https://github.com/cyverse-austria/ansible-k8s.git # navigate to cloned repo cd ansible-k8s","title":"Clone the repo"},{"location":"deployments/kubernetes-deploy/#create-your-inventory","text":"Create your inventory file under /inventory/MYINVENTORY and replace the host names as yours. [ k8s:children ] k8s-control-plane k8s-worker [ kube-apiserver-haproxy ] k8-haproxy [ k8s-control-plane ] k8-c01 k8-c02 [ k8s-storage:children ] k8s-worker [ k8s-worker:children ] vice-workers k8s-workers [ k8s-workers ] k8-w01 k8-w02 [ vice-workers ] k8-vice-w01 [ outward-facing-proxy ] vice-haproxy-01 [ haproxy ] [ loadbalancer ] haproxy-01","title":"create your inventory"},{"location":"deployments/kubernetes-deploy/#run-playbooks","text":"Note: we are using --user root --become , if your virtual machines have a diffrent user you could change this.","title":"Run playbooks"},{"location":"deployments/kubernetes-deploy/#check-if-your-hosts-are-reachable","text":"ansible -i /inventory/MYINVENTORY -m ping all --user root --become","title":"Check if your hosts are reachable"},{"location":"deployments/kubernetes-deploy/#setup-firewall-configs","text":"ansible-playbook -i /inventory/MYINVENTORY firewalld-config.yml --user root --become","title":"Setup firewall configs"},{"location":"deployments/kubernetes-deploy/#provision-your-nodes","text":"ansible-playbook -i /inventory/MYINVENTORY provision-nodes.yml --user root --become","title":"Provision your nodes"},{"location":"deployments/kubernetes-deploy/#init-masterworker-nodes","text":"ansible-playbook -i /inventory/MYINVENTORY multi-master.yml --user root --become","title":"Init master/worker nodes"},{"location":"deployments/kubernetes-deploy/#tainting-and-labeling-vice-worker-nodes","text":"kubectl label nodes k8-vice-w01 vice = true kubectl taint nodes k8-vice-w01 vice = only:NoSchedule","title":"Tainting and Labeling VICE Worker Nodes"},{"location":"deployments/local-exim/","text":"Local Exim (exim-sender) \u00b6 local-exim or also known as exim-sender Deploy \u00b6 ## deploy for prod env kubectl apply -f resources/deployments/exim-sender.yml -n prod ## deploy for prod discover # kubectl apply -f resources/deployments/exim-sender.yml -n discover","title":"Local Exim"},{"location":"deployments/local-exim/#local-exim-exim-sender","text":"local-exim or also known as exim-sender","title":"Local Exim (exim-sender)"},{"location":"deployments/local-exim/#deploy","text":"## deploy for prod env kubectl apply -f resources/deployments/exim-sender.yml -n prod ## deploy for prod discover # kubectl apply -f resources/deployments/exim-sender.yml -n discover","title":"Deploy"},{"location":"deployments/openebs/","text":"OpenEBS \u00b6 Install with Helm \u00b6 helm repo add openebs https://openebs.github.io/openebs helm repo update helm install openebs --namespace openebs openebs/openebs --create-namespace # If you do not want to install OpenEBS Replicated Storage, use the following command helm install openebs --namespace openebs openebs/openebs --set engines.replicated.mayastor.enabled = false --create-namespace Issues \u00b6 If encounter an issue regarding the openebs certificate expired: openebs admission-webhook.openebs.io\\\": Post \\\"https://admission-server-svc.openebs.svc:443/validate?timeout=5s\\\": x509: certificate has expired do the followings : kubectl delete secret admission-server-secret -n openebs kubectl delete validatingwebhookconfigurations.admissionregistration.k8s.io openebs-validation-webhook-cfg kubectl rollout restart deployment openebs-admission-server -n openebs","title":"OpenEBS"},{"location":"deployments/openebs/#openebs","text":"","title":"OpenEBS"},{"location":"deployments/openebs/#install-with-helm","text":"helm repo add openebs https://openebs.github.io/openebs helm repo update helm install openebs --namespace openebs openebs/openebs --create-namespace # If you do not want to install OpenEBS Replicated Storage, use the following command helm install openebs --namespace openebs openebs/openebs --set engines.replicated.mayastor.enabled = false --create-namespace","title":"Install with Helm"},{"location":"deployments/openebs/#issues","text":"If encounter an issue regarding the openebs certificate expired: openebs admission-webhook.openebs.io\\\": Post \\\"https://admission-server-svc.openebs.svc:443/validate?timeout=5s\\\": x509: certificate has expired do the followings : kubectl delete secret admission-server-secret -n openebs kubectl delete validatingwebhookconfigurations.admissionregistration.k8s.io openebs-validation-webhook-cfg kubectl rollout restart deployment openebs-admission-server -n openebs","title":"Issues"},{"location":"deployments/redis-ha/","text":"Redis HA \u00b6 In this Document we will cover installing the Redis Server and Redis Haproxy , due to the k8s-resources configurations both would be installed and configured inside the prod namespace. Preq \u00b6 Make sure to create a values.yaml and replace add your credentials: values.yaml Overrides the PersistentVolume, adds secrets and uses openebs as storage class. ## replicas number for each component replicas : 3 persistentVolume : enabled : true ## redis-ha data Persistent Volume Storage Class ## If defined, storageClassName: <storageClass> ## If set to \"-\", storageClassName: \"\", which disables dynamic provisioning ## If undefined (the default) or set to null, no storageClassName spec is ## set, choosing the default provisioner. (gp2 on AWS, standard on ## GKE, AWS & OpenStack) ## storageClass : openebs-hostpath ## Sentinel specific configuration options sentinel : auth : true authkey : <AUTH-KEY> password : <PASSWORD> ## Redis specific configuration options auth : true authkey : <AUTH-KEY> redisPassword : <PASSWORD> Redis Server & Sentinel \u00b6 # add helm repo helm repo add dandydev https://dandydeveloper.github.io/charts helm repo update ## deploy redis servers for prod env helm upgrade --install --namespace prod redis-ha dandydev/redis-ha --values values.yaml ## deploy redis server for discover env # helm upgrade --install --namespace discover redis-ha dandydev/redis-ha --values values.yaml Redis Haproxy \u00b6 Preq \u00b6 Make sure to load the configs/secrets see also k8s-resources Deploy redis-haproxy \u00b6 ## deploy for prod env kubectl apply -n prod -f resources/deployments/redis-haproxy.yml ## deploy for discover env # kubectl apply -n discover -f resources/deployments/redis-haproxy.yml","title":"Redis HA"},{"location":"deployments/redis-ha/#redis-ha","text":"In this Document we will cover installing the Redis Server and Redis Haproxy , due to the k8s-resources configurations both would be installed and configured inside the prod namespace.","title":"Redis HA"},{"location":"deployments/redis-ha/#preq","text":"Make sure to create a values.yaml and replace add your credentials: values.yaml Overrides the PersistentVolume, adds secrets and uses openebs as storage class. ## replicas number for each component replicas : 3 persistentVolume : enabled : true ## redis-ha data Persistent Volume Storage Class ## If defined, storageClassName: <storageClass> ## If set to \"-\", storageClassName: \"\", which disables dynamic provisioning ## If undefined (the default) or set to null, no storageClassName spec is ## set, choosing the default provisioner. (gp2 on AWS, standard on ## GKE, AWS & OpenStack) ## storageClass : openebs-hostpath ## Sentinel specific configuration options sentinel : auth : true authkey : <AUTH-KEY> password : <PASSWORD> ## Redis specific configuration options auth : true authkey : <AUTH-KEY> redisPassword : <PASSWORD>","title":"Preq"},{"location":"deployments/redis-ha/#redis-server-sentinel","text":"# add helm repo helm repo add dandydev https://dandydeveloper.github.io/charts helm repo update ## deploy redis servers for prod env helm upgrade --install --namespace prod redis-ha dandydev/redis-ha --values values.yaml ## deploy redis server for discover env # helm upgrade --install --namespace discover redis-ha dandydev/redis-ha --values values.yaml","title":"Redis Server &amp; Sentinel"},{"location":"deployments/redis-ha/#redis-haproxy","text":"","title":"Redis Haproxy"},{"location":"deployments/redis-ha/#preq_1","text":"Make sure to load the configs/secrets see also k8s-resources","title":"Preq"},{"location":"deployments/redis-ha/#deploy-redis-haproxy","text":"## deploy for prod env kubectl apply -n prod -f resources/deployments/redis-haproxy.yml ## deploy for discover env # kubectl apply -n discover -f resources/deployments/redis-haproxy.yml","title":"Deploy redis-haproxy"},{"location":"deployments/unleash/","text":"Unleash \u00b6 Preq \u00b6 Make sure unleash database is setup, see also unleash-db Deploy \u00b6 ## Deploy for prod env kubectl apply -f resources/deployments/unleash.yml -n prod ## Deploy for discover env # kubectl apply -f resources/deployments/unleash.yml -n discover","title":"Unleash"},{"location":"deployments/unleash/#unleash","text":"","title":"Unleash"},{"location":"deployments/unleash/#preq","text":"Make sure unleash database is setup, see also unleash-db","title":"Preq"},{"location":"deployments/unleash/#deploy","text":"## Deploy for prod env kubectl apply -f resources/deployments/unleash.yml -n prod ## Deploy for discover env # kubectl apply -f resources/deployments/unleash.yml -n discover","title":"Deploy"},{"location":"deployments/userportal/","text":"User Portal \u00b6 Prerequisites \u00b6 Before we dive in to the deploying the user-portal, please make sure that you have the following configuration in place. LDAP \u00b6 It is required that we have a user in LDAP - so the user-portal can create/edit users. create portal user \u00b6 Create portal-user.ldif \u00b6 dn : uid = portal , ou = People , dc = tugraz , dc = at objectClass : inetOrgPerson objectClass : posixAccount objectClass : shadowAccount uid : portal mail : portal@example.com sn : SeviceAccount givenName : PORTAL cn : portal title : Other o : N/A departmentNumber : N/A uidNumber : 40003 gidNumber : 10003 homeDirectory : /home/portal Apply ldap file \u00b6 # replace LDAP_PASSWORD ldapadd -x -D \"cn=Manager,dc=tugraz,dc=at\" -w \"LDAP_PASSWORD\" -f portal-user.ldif # create a password for the user ## replace PORTAL_PASSWORD & LDAP_PASSWORD ldappasswd -x -D cn = Manager,dc = tugraz,dc = at -w \"LDAP_PASSWORD\" -s \"PORTAL_PASSWORD\" \"uid=portal,ou=People,dc=tugraz,dc=at\" Add portal user to admim group \u00b6 Create portal-de_admin.ldif \u00b6 dn : cn = de_admins , ou = Groups , dc = tugraz , dc = at changetype : modify add : memberUid memberUid : portal Apply ldap file \u00b6 # replace LDAP_PASSWORD ## add user to the de_admin group ldapmodify -x -D \"cn=Manager,dc=tugraz,dc=at\" -w \"LDAP_PASSWORD\" -f portal-de_admin.ldif iRODs \u00b6 For user-portal we would need to create an irods rodsadmin user. # replace YOURPASSWORDHERE ## su - irods iadmin mkuser portal rodsadmin iadmin moduser portal password YOURPASSWORDHERE Database \u00b6 For Database configuration and setup vist portal-db docs . Deploy \u00b6 The kustomize manifests can be found in repository https://github.com/cyverse-austria/portal you can clone this repository and follow the README guide. # create ns kubectl create ns user-portal # deploy kubectl apply -k /base -n user-portal","title":"UserPortal"},{"location":"deployments/userportal/#user-portal","text":"","title":"User Portal"},{"location":"deployments/userportal/#prerequisites","text":"Before we dive in to the deploying the user-portal, please make sure that you have the following configuration in place.","title":"Prerequisites"},{"location":"deployments/userportal/#ldap","text":"It is required that we have a user in LDAP - so the user-portal can create/edit users.","title":"LDAP"},{"location":"deployments/userportal/#create-portal-user","text":"","title":"create portal user"},{"location":"deployments/userportal/#create-portal-userldif","text":"dn : uid = portal , ou = People , dc = tugraz , dc = at objectClass : inetOrgPerson objectClass : posixAccount objectClass : shadowAccount uid : portal mail : portal@example.com sn : SeviceAccount givenName : PORTAL cn : portal title : Other o : N/A departmentNumber : N/A uidNumber : 40003 gidNumber : 10003 homeDirectory : /home/portal","title":"Create portal-user.ldif"},{"location":"deployments/userportal/#apply-ldap-file","text":"# replace LDAP_PASSWORD ldapadd -x -D \"cn=Manager,dc=tugraz,dc=at\" -w \"LDAP_PASSWORD\" -f portal-user.ldif # create a password for the user ## replace PORTAL_PASSWORD & LDAP_PASSWORD ldappasswd -x -D cn = Manager,dc = tugraz,dc = at -w \"LDAP_PASSWORD\" -s \"PORTAL_PASSWORD\" \"uid=portal,ou=People,dc=tugraz,dc=at\"","title":"Apply ldap file"},{"location":"deployments/userportal/#add-portal-user-to-admim-group","text":"","title":"Add portal user to admim group"},{"location":"deployments/userportal/#create-portal-de_adminldif","text":"dn : cn = de_admins , ou = Groups , dc = tugraz , dc = at changetype : modify add : memberUid memberUid : portal","title":"Create portal-de_admin.ldif"},{"location":"deployments/userportal/#apply-ldap-file_1","text":"# replace LDAP_PASSWORD ## add user to the de_admin group ldapmodify -x -D \"cn=Manager,dc=tugraz,dc=at\" -w \"LDAP_PASSWORD\" -f portal-de_admin.ldif","title":"Apply ldap file"},{"location":"deployments/userportal/#irods","text":"For user-portal we would need to create an irods rodsadmin user. # replace YOURPASSWORDHERE ## su - irods iadmin mkuser portal rodsadmin iadmin moduser portal password YOURPASSWORDHERE","title":"iRODs"},{"location":"deployments/userportal/#database","text":"For Database configuration and setup vist portal-db docs .","title":"Database"},{"location":"deployments/userportal/#deploy","text":"The kustomize manifests can be found in repository https://github.com/cyverse-austria/portal you can clone this repository and follow the README guide. # create ns kubectl create ns user-portal # deploy kubectl apply -k /base -n user-portal","title":"Deploy"},{"location":"deployments/vice/","text":"vice apps \u00b6 Create namespace \u00b6 kubectl create ns vice-apps Create a secret on namespace vice-apps vice-image-pull-secret \u00b6 This secret has the harbor.org which allows the pod to pull docker images. kubectl create secret generic vice-image-pull-secret \\ --from-file = .dockerconfigjson = /root/.docker/config.json \\ --type = kubernetes.io/dockerconfigjson -n vice-apps Create serviceAccounts \u00b6 ## edit the namespace if you are running in a diffrent env ## vi /k8s-resources/resources/serviceaccounts/app-exposer.yml kubectl apply -f /k8s-resources/resources/serviceaccounts/app-exposer.yml kubectl apply -f /k8s-resources/resources/serviceaccounts/vice-app-runner.yml Apply clusterrolebindings \u00b6 ## edit the namespace if you are running in a diffrent env ## vi /k8s-resources/resources/clusterrolebindings/app-exposer.yml kubectl apply -f /k8s-resources/resources/clusterrolebindings/app-exposer.yml Apply networkpolicies \u00b6 To apply networkpolicies we need to edit the file /k8s-resources/resources/networkpolicies/vice-apps.yml , and add all the worker nodes, and master nodes, to it if we are using a diffrent env rather than prod . e.g. - except: - ******** - ******** + except: - 10.0.10.0/24 # k8s master CIDR - ****************/32 # c1 - ****************/32 # w1 - ***************/32 # w2 - ***************/32 # w3 - **************/32 # w4 - *************/32 # w5 - ************/32 # vice-w1 Run policy kubectl apply -f /k8s-resources/resources/networkpolicies/vice-apps.yml Apply roles \u00b6 kubectl apply -f /k8s-resources/resources/roles/vice-apps.yml Create porklock-config secrert for irods \u00b6 Create irods-config.properties \u00b6 Create a file irods-config.properties and add the values. porklock.irods-home = porklock.irods-user = porklock.irods-pass = porklock.irods-host = porklock.irods-port = porklock.irods-zone = porklock.irods-resc = Create secret from file \u00b6 kubectl -n vice-apps create secret generic porklock-config --from-file = irods-config.properties Restart the services \u00b6 kubectl rollout restart apps app-exposer templeton-incremental templeton-periodic -n NAMESPACE Install/configure ingress-nginx \u00b6 For installing and configuring the ingress-nginx have a look at ingress-nginx","title":"Vice"},{"location":"deployments/vice/#vice-apps","text":"","title":"vice apps"},{"location":"deployments/vice/#create-namespace","text":"kubectl create ns vice-apps","title":"Create namespace"},{"location":"deployments/vice/#create-a-secret-on-namespace-vice-apps-vice-image-pull-secret","text":"This secret has the harbor.org which allows the pod to pull docker images. kubectl create secret generic vice-image-pull-secret \\ --from-file = .dockerconfigjson = /root/.docker/config.json \\ --type = kubernetes.io/dockerconfigjson -n vice-apps","title":"Create a secret on namespace vice-apps vice-image-pull-secret"},{"location":"deployments/vice/#create-serviceaccounts","text":"## edit the namespace if you are running in a diffrent env ## vi /k8s-resources/resources/serviceaccounts/app-exposer.yml kubectl apply -f /k8s-resources/resources/serviceaccounts/app-exposer.yml kubectl apply -f /k8s-resources/resources/serviceaccounts/vice-app-runner.yml","title":"Create serviceAccounts"},{"location":"deployments/vice/#apply-clusterrolebindings","text":"## edit the namespace if you are running in a diffrent env ## vi /k8s-resources/resources/clusterrolebindings/app-exposer.yml kubectl apply -f /k8s-resources/resources/clusterrolebindings/app-exposer.yml","title":"Apply clusterrolebindings"},{"location":"deployments/vice/#apply-networkpolicies","text":"To apply networkpolicies we need to edit the file /k8s-resources/resources/networkpolicies/vice-apps.yml , and add all the worker nodes, and master nodes, to it if we are using a diffrent env rather than prod . e.g. - except: - ******** - ******** + except: - 10.0.10.0/24 # k8s master CIDR - ****************/32 # c1 - ****************/32 # w1 - ***************/32 # w2 - ***************/32 # w3 - **************/32 # w4 - *************/32 # w5 - ************/32 # vice-w1 Run policy kubectl apply -f /k8s-resources/resources/networkpolicies/vice-apps.yml","title":"Apply networkpolicies"},{"location":"deployments/vice/#apply-roles","text":"kubectl apply -f /k8s-resources/resources/roles/vice-apps.yml","title":"Apply roles"},{"location":"deployments/vice/#create-porklock-config-secrert-for-irods","text":"","title":"Create porklock-config secrert for irods"},{"location":"deployments/vice/#create-irods-configproperties","text":"Create a file irods-config.properties and add the values. porklock.irods-home = porklock.irods-user = porklock.irods-pass = porklock.irods-host = porklock.irods-port = porklock.irods-zone = porklock.irods-resc =","title":"Create irods-config.properties"},{"location":"deployments/vice/#create-secret-from-file","text":"kubectl -n vice-apps create secret generic porklock-config --from-file = irods-config.properties","title":"Create secret from file"},{"location":"deployments/vice/#restart-the-services","text":"kubectl rollout restart apps app-exposer templeton-incremental templeton-periodic -n NAMESPACE","title":"Restart the services"},{"location":"deployments/vice/#installconfigure-ingress-nginx","text":"For installing and configuring the ingress-nginx have a look at ingress-nginx","title":"Install/configure ingress-nginx"},{"location":"development/","text":"coming soon \u00b6","title":"coming soon"},{"location":"development/#coming-soon","text":"","title":"coming soon"},{"location":"others/main/","text":"Post-Deployment Steps \u00b6 1. Post-Deployment iRODS \u00b6 Gain Access to the Host and Switch to the iRODS Admin User \u00b6 ssh <irods_user>@IRODS.HOST sudo su - irods Create iRODS Account for de-irods \u00b6 To create the user de-irods and set the password: iadmin mkuser de-irods rodsadmin iadmin moduser de-irods password DE_USER_PASSWORD Add de-irods to the rodsadmin Group \u00b6 Add the user de-irods to the rodsadmin group: iadmin atg rodsadmin de-irods Grant Ownership of /TUG/home/shared to rodsadmin \u00b6 Ensure that rodsadmin owns the specified directory: ichmod own rodsadmin /TUG/home/shared Grant Read Access to Public \u00b6 Grant public read access to the home and shared directories: ichmod read public /TUG/home ichmod read public /TUG/home/shared Grant icat_reader Database Permissions \u00b6 To grant the icat_reader user the necessary database permissions, run the following SQL command: --- \\c ICAT GRANT SELECT , INSERT , UPDATE , DELETE ON ALL TABLES IN SCHEMA \"public\" TO icat_reader ; Crontab data-store-fix \u00b6 This program fixes the following three common problems that happen when an upload fails. The rodsadmin group isn't given own permission on a new collection or data object. A new collection or data object doesn't receive a UUID. A checksum isn't computed for a new or modified data object replica. Gain Access to the Host and Switch to the iRODS Admin User \u00b6 ssh <irods_user>@IRODS.HOST sudo su - irods Create a directory \u00b6 mkdir -p /var/lib/ds-adm cd /var/lib/ds-adm Clone the script repo \u00b6 git clone https://github.com/cyverse-austria/irods-adm.git Run Manually \u00b6 cd /var/lib/ds-adm/irods-adm ./data-store-fix --db-user $ICAT_DB_USER --dbms-host $ICAT_DB_HOST create a crontab \u00b6 Make sure SMTP is enabled. MAILFROM = ds-adm MAILTO = root,yourmail@domain.com PGHOST = ICAT_DB_HOST PGUSER = $ICAT_DB_USER #m h dom mon dow command 0 1 * * * /var/lib/ds-adm/irods-adm/data-store-fix --db-user $ICAT_DB_USER --dbms-host $ICAT_DB_HOST 2. Post-Deployment DE(Discovery Environment) \u00b6 Create iRODS Account for portal \u00b6 To create the user portal and set the password: iadmin mkuser portal rodsadmin iadmin moduser portal password PORTAL_PASSWORD Add portal to the rodsadmin Group \u00b6 Add the portal user to the rodsadmin group: iadmin atg rodsadmin portal Add User to Admin Group (LDAP) \u00b6 Follow these steps to add an existing user to the de-admins group in LDAP. Step 1: Create a LDIF File to Add the User \u00b6 Create an LDIF file named add-de_admins.ldif to add a user to the de-admins LDAP group. Replace YOUR_USER_NAME with the actual username of the user you want to add. dn : cn = de_admins , ou = Groups , dc = tugraz , dc = at changetype : modify add : memberuid memberuid : YOUR_USER_NAME Step 2: Run the LDAP Modify Command \u00b6 Run the following command to apply the changes and add the user to the LDAP group: read -s PASSWORD && export PASSWORD ldapmodify -x -D \"cn=Manager,dc=tugraz,dc=at\" -w $PASSWORD -f add-de_admins.ldif \u2705 Create Anonymous User Workspace for Apps \u00b6 Ensure that your LDAP configuration includes an anonymous user and that this user is added to the everyone group. \ud83d\udc33 Run a Temporary Debian Container in Your Namespace \u00b6 kubectl -n $NAMESPACE run testing \\ --rm -it \\ --image = debian:stable-slim \\ -- bash Install curl Inside the Container \u00b6 apt-get update && apt-get install curl Trigger Workspace Creation for the Anonymous User \u00b6 curl \"http://apps/bootstrap?user=anonymous\" User Provisioning: iRODS + LDAP \u00b6 This guide explains how to create a new user in both iRODS and OpenLDAP , including group membership and password setup. 1. Create iRODS User Account \u00b6 Run the following commands as an iRODS administrator: iadmin mkuser user01 rodsuser iadmin moduser user01 password PASSWORD This creates an iRODS user user01 with the type rodsuser and sets the password to PASSWORD . 2. Create LDAP User Account \u00b6 Step 1: Create an LDIF file for the new user \u00b6 Example: testuser.ldif dn : uid = user01 , ou = People , dc = tugraz , dc = at objectClass : inetOrgPerson objectClass : posixAccount objectClass : shadowAccount uid : user01 uidNumber : 40005 gidNumber : 10009 homeDirectory : /home/user01 mail : user01@cyverse.at sn : surname givenName : Test cn : Test Surname title : University/College Staff o : Graz University of Technology Step 2: Add the user to LDAP \u00b6 ldapadd -x -D \"cn=Manager,dc=tugraz,dc=at\" -w \" $MANAGER_PASSWORD \" -f testuser.ldif 3. Set LDAP Password for the User \u00b6 ldappasswd -x \\ -D \"cn=Manager,dc=tugraz,dc=at\" \\ -w \" $MANAGER_PASSWORD \" \\ -s \"PASSWORD\" \\ \"uid=user01,ou=People,dc=tugraz,dc=at\" 4. Add User to everyone Group \u00b6 Step 1: Create an LDIF file for group modification \u00b6 Example: add-everyone.ldif dn : cn = everyone , ou = Groups , dc = tugraz , dc = at changetype : modify add : memberuid memberuid : user01 Step 2: Apply the group modification \u00b6 ldapmodify -x -D \"cn=Manager,dc=tugraz,dc=at\" -w \" $MANAGER_PASSWORD \" -f add-everyone.ldif 5. Add User to community Group \u00b6 Step 1: Create an LDIF file for group modification \u00b6 Example: add-community.ldif dn : cn = community , ou = Groups , dc = tugraz , dc = at changetype : modify add : memberuid memberUid : user01 Step 2: Apply the group modification \u00b6 ldapmodify -x -D \"cn=Manager,dc=tugraz,dc=at\" -w \" $MANAGER_PASSWORD \" -f add-community.ldif","title":"Others"},{"location":"others/main/#post-deployment-steps","text":"","title":"Post-Deployment Steps"},{"location":"others/main/#1-post-deployment-irods","text":"","title":"1. Post-Deployment iRODS"},{"location":"others/main/#gain-access-to-the-host-and-switch-to-the-irods-admin-user","text":"ssh <irods_user>@IRODS.HOST sudo su - irods","title":"Gain Access to the Host and Switch to the iRODS Admin User"},{"location":"others/main/#create-irods-account-for-de-irods","text":"To create the user de-irods and set the password: iadmin mkuser de-irods rodsadmin iadmin moduser de-irods password DE_USER_PASSWORD","title":"Create iRODS Account for de-irods"},{"location":"others/main/#add-de-irods-to-the-rodsadmin-group","text":"Add the user de-irods to the rodsadmin group: iadmin atg rodsadmin de-irods","title":"Add de-irods to the rodsadmin Group"},{"location":"others/main/#grant-ownership-of-tughomeshared-to-rodsadmin","text":"Ensure that rodsadmin owns the specified directory: ichmod own rodsadmin /TUG/home/shared","title":"Grant Ownership of /TUG/home/shared to rodsadmin"},{"location":"others/main/#grant-read-access-to-public","text":"Grant public read access to the home and shared directories: ichmod read public /TUG/home ichmod read public /TUG/home/shared","title":"Grant Read Access to Public"},{"location":"others/main/#grant-icat_reader-database-permissions","text":"To grant the icat_reader user the necessary database permissions, run the following SQL command: --- \\c ICAT GRANT SELECT , INSERT , UPDATE , DELETE ON ALL TABLES IN SCHEMA \"public\" TO icat_reader ;","title":"Grant icat_reader Database Permissions"},{"location":"others/main/#crontab-data-store-fix","text":"This program fixes the following three common problems that happen when an upload fails. The rodsadmin group isn't given own permission on a new collection or data object. A new collection or data object doesn't receive a UUID. A checksum isn't computed for a new or modified data object replica.","title":"Crontab data-store-fix"},{"location":"others/main/#gain-access-to-the-host-and-switch-to-the-irods-admin-user_1","text":"ssh <irods_user>@IRODS.HOST sudo su - irods","title":"Gain Access to the Host and Switch to the iRODS Admin User"},{"location":"others/main/#create-a-directory","text":"mkdir -p /var/lib/ds-adm cd /var/lib/ds-adm","title":"Create a directory"},{"location":"others/main/#clone-the-script-repo","text":"git clone https://github.com/cyverse-austria/irods-adm.git","title":"Clone the script repo"},{"location":"others/main/#run-manually","text":"cd /var/lib/ds-adm/irods-adm ./data-store-fix --db-user $ICAT_DB_USER --dbms-host $ICAT_DB_HOST","title":"Run Manually"},{"location":"others/main/#create-a-crontab","text":"Make sure SMTP is enabled. MAILFROM = ds-adm MAILTO = root,yourmail@domain.com PGHOST = ICAT_DB_HOST PGUSER = $ICAT_DB_USER #m h dom mon dow command 0 1 * * * /var/lib/ds-adm/irods-adm/data-store-fix --db-user $ICAT_DB_USER --dbms-host $ICAT_DB_HOST","title":"create a crontab"},{"location":"others/main/#2-post-deployment-dediscovery-environment","text":"","title":"2. Post-Deployment DE(Discovery Environment)"},{"location":"others/main/#create-irods-account-for-portal","text":"To create the user portal and set the password: iadmin mkuser portal rodsadmin iadmin moduser portal password PORTAL_PASSWORD","title":"Create iRODS Account for portal"},{"location":"others/main/#add-portal-to-the-rodsadmin-group","text":"Add the portal user to the rodsadmin group: iadmin atg rodsadmin portal","title":"Add portal to the rodsadmin Group"},{"location":"others/main/#add-user-to-admin-group-ldap","text":"Follow these steps to add an existing user to the de-admins group in LDAP.","title":"Add User to Admin Group (LDAP)"},{"location":"others/main/#step-1-create-a-ldif-file-to-add-the-user","text":"Create an LDIF file named add-de_admins.ldif to add a user to the de-admins LDAP group. Replace YOUR_USER_NAME with the actual username of the user you want to add. dn : cn = de_admins , ou = Groups , dc = tugraz , dc = at changetype : modify add : memberuid memberuid : YOUR_USER_NAME","title":"Step 1: Create a LDIF File to Add the User"},{"location":"others/main/#step-2-run-the-ldap-modify-command","text":"Run the following command to apply the changes and add the user to the LDAP group: read -s PASSWORD && export PASSWORD ldapmodify -x -D \"cn=Manager,dc=tugraz,dc=at\" -w $PASSWORD -f add-de_admins.ldif","title":"Step 2: Run the LDAP Modify Command"},{"location":"others/main/#create-anonymous-user-workspace-for-apps","text":"Ensure that your LDAP configuration includes an anonymous user and that this user is added to the everyone group.","title":"\u2705 Create Anonymous User Workspace for Apps"},{"location":"others/main/#run-a-temporary-debian-container-in-your-namespace","text":"kubectl -n $NAMESPACE run testing \\ --rm -it \\ --image = debian:stable-slim \\ -- bash","title":"\ud83d\udc33 Run a Temporary Debian Container in Your Namespace"},{"location":"others/main/#install-curl-inside-the-container","text":"apt-get update && apt-get install curl","title":"Install curl Inside the Container"},{"location":"others/main/#trigger-workspace-creation-for-the-anonymous-user","text":"curl \"http://apps/bootstrap?user=anonymous\"","title":"Trigger Workspace Creation for the Anonymous User"},{"location":"others/main/#user-provisioning-irods-ldap","text":"This guide explains how to create a new user in both iRODS and OpenLDAP , including group membership and password setup.","title":"User Provisioning: iRODS + LDAP"},{"location":"others/main/#1-create-irods-user-account","text":"Run the following commands as an iRODS administrator: iadmin mkuser user01 rodsuser iadmin moduser user01 password PASSWORD This creates an iRODS user user01 with the type rodsuser and sets the password to PASSWORD .","title":"1. Create iRODS User Account"},{"location":"others/main/#2-create-ldap-user-account","text":"","title":"2. Create LDAP User Account"},{"location":"others/main/#step-1-create-an-ldif-file-for-the-new-user","text":"Example: testuser.ldif dn : uid = user01 , ou = People , dc = tugraz , dc = at objectClass : inetOrgPerson objectClass : posixAccount objectClass : shadowAccount uid : user01 uidNumber : 40005 gidNumber : 10009 homeDirectory : /home/user01 mail : user01@cyverse.at sn : surname givenName : Test cn : Test Surname title : University/College Staff o : Graz University of Technology","title":"Step 1: Create an LDIF file for the new user"},{"location":"others/main/#step-2-add-the-user-to-ldap","text":"ldapadd -x -D \"cn=Manager,dc=tugraz,dc=at\" -w \" $MANAGER_PASSWORD \" -f testuser.ldif","title":"Step 2: Add the user to LDAP"},{"location":"others/main/#3-set-ldap-password-for-the-user","text":"ldappasswd -x \\ -D \"cn=Manager,dc=tugraz,dc=at\" \\ -w \" $MANAGER_PASSWORD \" \\ -s \"PASSWORD\" \\ \"uid=user01,ou=People,dc=tugraz,dc=at\"","title":"3. Set LDAP Password for the User"},{"location":"others/main/#4-add-user-to-everyone-group","text":"","title":"4. Add User to everyone Group"},{"location":"others/main/#step-1-create-an-ldif-file-for-group-modification","text":"Example: add-everyone.ldif dn : cn = everyone , ou = Groups , dc = tugraz , dc = at changetype : modify add : memberuid memberuid : user01","title":"Step 1: Create an LDIF file for group modification"},{"location":"others/main/#step-2-apply-the-group-modification","text":"ldapmodify -x -D \"cn=Manager,dc=tugraz,dc=at\" -w \" $MANAGER_PASSWORD \" -f add-everyone.ldif","title":"Step 2: Apply the group modification"},{"location":"others/main/#5-add-user-to-community-group","text":"","title":"5. Add User to community Group"},{"location":"others/main/#step-1-create-an-ldif-file-for-group-modification_1","text":"Example: add-community.ldif dn : cn = community , ou = Groups , dc = tugraz , dc = at changetype : modify add : memberuid memberUid : user01","title":"Step 1: Create an LDIF file for group modification"},{"location":"others/main/#step-2-apply-the-group-modification_1","text":"ldapmodify -x -D \"cn=Manager,dc=tugraz,dc=at\" -w \" $MANAGER_PASSWORD \" -f add-community.ldif","title":"Step 2: Apply the group modification"},{"location":"services/","text":"main \u00b6","title":"main"},{"location":"services/#main","text":"","title":"main"},{"location":"services/harbor/","text":"Harbor Registry \u2014 Push Image Guide \u00b6 Overview \u00b6 Harbor is an open-source container registry that provides: Role-Based Access Control (RBAC) Integration with SSO / OIDC (via Keycloak) Vulnerability scanning and secure artifact storage This guide shows you how to get started with Harbor, create your account via SSO, and push Docker images to your assigned project. 1. Getting a Harbor Account \u00b6 Go to the Harbor Web Interface Log in via SSO (Keycloak OIDC) using your CyVerse Austria credentials. After your first login, your Harbor account is automatically provisioned: You\u2019ll automatically get your designated role within that project (e.g. Developer , Maintainer ). 2. Docker Login & Using Harbor \u00b6 Once your account is ready, follow these steps to log in and push your images. Step 1: Log in via Docker CLI \u00b6 docker login harbor.<your_domain> You\u2019ll be prompted for username and password . Username: Your Harbor username (same as your Keycloak username) Password: Use your CLI secret (not your Keycloak password) To find your CLI secret: Log into the Harbor UI Click your user profile (top right corner) Go to User Profile \u2192 CLI Secret Copy and use it when prompted for your password Step 2: Tag Your Image \u00b6 Replace the placeholders with your project and image name: docker tag <your_image> harbor.<your_domain>/<your_project>/<image_name>:<tag> Step 3: Push Your Image \u00b6 docker push harbor.<your_domain>/<your_project>/<image_name>:<tag>","title":"harbor"},{"location":"services/harbor/#harbor-registry-push-image-guide","text":"","title":"Harbor Registry \u2014 Push Image Guide"},{"location":"services/harbor/#overview","text":"Harbor is an open-source container registry that provides: Role-Based Access Control (RBAC) Integration with SSO / OIDC (via Keycloak) Vulnerability scanning and secure artifact storage This guide shows you how to get started with Harbor, create your account via SSO, and push Docker images to your assigned project.","title":"Overview"},{"location":"services/harbor/#1-getting-a-harbor-account","text":"Go to the Harbor Web Interface Log in via SSO (Keycloak OIDC) using your CyVerse Austria credentials. After your first login, your Harbor account is automatically provisioned: You\u2019ll automatically get your designated role within that project (e.g. Developer , Maintainer ).","title":"1. Getting a Harbor Account"},{"location":"services/harbor/#2-docker-login-using-harbor","text":"Once your account is ready, follow these steps to log in and push your images.","title":"2. Docker Login &amp; Using Harbor"},{"location":"services/harbor/#step-1-log-in-via-docker-cli","text":"docker login harbor.<your_domain> You\u2019ll be prompted for username and password . Username: Your Harbor username (same as your Keycloak username) Password: Use your CLI secret (not your Keycloak password) To find your CLI secret: Log into the Harbor UI Click your user profile (top right corner) Go to User Profile \u2192 CLI Secret Copy and use it when prompted for your password","title":"Step 1: Log in via Docker CLI"},{"location":"services/harbor/#step-2-tag-your-image","text":"Replace the placeholders with your project and image name: docker tag <your_image> harbor.<your_domain>/<your_project>/<image_name>:<tag>","title":"Step 2: Tag Your Image"},{"location":"services/harbor/#step-3-push-your-image","text":"docker push harbor.<your_domain>/<your_project>/<image_name>:<tag>","title":"Step 3: Push Your Image"},{"location":"services/keycloak/realm/","text":"todo \u00b6 realm clients","title":"todo"},{"location":"services/keycloak/realm/#todo","text":"realm clients","title":"todo"},{"location":"services/kubernetes/","text":"Services \u00b6 Here you will find all the services which are running on a Kubernetes Cluster. List of core-service \u00b6 analyses : Provides a HTTP API for interacting with analyses in the Discovery Environment. app-exposer : This is a service that runs inside of a Kubernetes cluster namespace and implements CRUD operations for exposing interactive apps as a Service and Endpoint. apply-labels : A small service in the Discovery Environment backend that periodically hits the app-exposer service to trigger the application of labels on VICE-related K8s resources apps : apps is a platform for hosting App Services for the Discovery Environment web application. async-tasks : This service tracks and manages asynchronous tasks throughout the DE backend services. bulk-typer : Like info-typer, but a bunch at once. hopefully. check-resource-access : Looks up the permissions that a subject has for a resource. By default, the subject type is 'user' and the resource type is 'analysis'. Only performs look ups against the permissions service. clockwork : Scheduled jobs for the CyVerse Discovery Environment. dashboard-aggregator : Gathers data to populate the dashboard in Sonora. data-info : data-info is a RESTful frontend for getting information about and manipulating information in an iRODS data store. data-usage-api : A service that provides an API around data usage tracking, and updates data usage numbers on request or periodically. de-mailer : A go module that send email notifications to users. This module will support HTML and rich text emails. de-stats : Service for obtaining CyVerse Discovery Environment stats and metrics. de-webhooks : A service that listens to AMQP queues for DE notifications, check if the user has webhooks defined for that notification type and then post the notification to webhook if one is defined. dewey : An AMQP message based iRODS indexer for elasticsearch. discoenv-analyses : A small service that makes analysis-related information available over NATS. discoenv-users : Is a microservice for the CyVerse Discovery Environment which allows users to look up information about a user. It uses NATS for communication. email-requests : A simple service to wait for email requests to arrive over AMQP and forward them to cyverse-email. event-recorder : This service listens to an AMQP topic for events, and records events that may be of interest to users in the notifications database. get-analysis-id : Microservice that uses the DE's apps service to return the analysis UUID associated with the provided external UUID. grouper : - grouper-loader & grouper-ws TODO: FIND the repo. - From kubectl apply group-propagator : info-typer : An AMQP message based info type detector infosquito2 : TODO FIND description. iplant-groups : A RESTful facade in front of Grouper . jex-adapter : TODO FIND description. job-status-recorder : TODO FIND description. job-status-listener : Listens over HTTP for job status updates, then publishes them to AMQP. job-status-to-apps-adapter : kifshare : A simple web page that allows users to download publicly available files without a CyVerse account. local-exim (exim-sender) : TODO: FIND the repo. - From kubectl metadata : The REST API for the Discovery Environment Metadata services. monkey : This is a service that synchronizes the tag documents in the data search index with the metadata database monitoring-agent : A service for the Discovery Environment that checks on the state of various internal and external resources and reports that state back to a central data aggregation service for later export to Prometheus. notifications : This service provides the RESTful API for the revised notification system. permissions : This service manages permissions for the Discovery environment. requests : Service for managing administrative requests in the CyVerse Discovery Environment. terrain : Terrain provides the primary REST API used by the Discovery Environment. Its role is to validate user authentication and to coordinate calls to other web services. For more information, please see the Discovery Environment API Documentation . resource-usage-api : is a microservice developed as part of the CyVerse Discovery Environment that provides access to resource usage values (CPU hours, memory, etc.) consumed by users over a customizable time period. saved-searches : A service for the CyVerse Discovery Environment that provides CRUD access to a user's saved searches. search : This is a service which serves as a search facade for the DE and others to use. It uses the querydsl library under the covers to translate requests and provide documentation, then passes off queries to configured elasticsearch servers. sonora : UI for the Discovery Environment templeton : includes templeton-incremental & templeton-periodic TODO: add description. timelord : timelord periodically queries the DE database for running applications and kills any of them that have gone over their time limit. unleash : TODO: FIND the repo. - From kubectl apply user-info : A service for getting user-related information like sessions and preferences. vice-default-backend : Provides a default backend handler for the Kubernetes Ingress that handles routing for VICE apps. This backend decides whether to redirect requests to the loading page service, the landing page service, or to a 404 page depending on whether the URL is valid or not. qms-adapter : Forwards usage information gathered within the Discovery Environment to the Quota Management System QMS . qms : QMS is the CyVerse Quota Management System. Its purpose is to keep track of resource usage limits and totals for CyVerse users. irods-csi-driver : iRODS Container Storage Interface (CSI) Driver implements the CSI Specification to provide container orchestration engines (like Kubernetes) iRODS access. dataone-indexer : Event indexer for the DataONE member node service. List of non-core services \u00b6 redis-ha redis-haproxy elasticsearch keycloak openebs","title":"Services"},{"location":"services/kubernetes/#services","text":"Here you will find all the services which are running on a Kubernetes Cluster.","title":"Services"},{"location":"services/kubernetes/#list-of-core-service","text":"analyses : Provides a HTTP API for interacting with analyses in the Discovery Environment. app-exposer : This is a service that runs inside of a Kubernetes cluster namespace and implements CRUD operations for exposing interactive apps as a Service and Endpoint. apply-labels : A small service in the Discovery Environment backend that periodically hits the app-exposer service to trigger the application of labels on VICE-related K8s resources apps : apps is a platform for hosting App Services for the Discovery Environment web application. async-tasks : This service tracks and manages asynchronous tasks throughout the DE backend services. bulk-typer : Like info-typer, but a bunch at once. hopefully. check-resource-access : Looks up the permissions that a subject has for a resource. By default, the subject type is 'user' and the resource type is 'analysis'. Only performs look ups against the permissions service. clockwork : Scheduled jobs for the CyVerse Discovery Environment. dashboard-aggregator : Gathers data to populate the dashboard in Sonora. data-info : data-info is a RESTful frontend for getting information about and manipulating information in an iRODS data store. data-usage-api : A service that provides an API around data usage tracking, and updates data usage numbers on request or periodically. de-mailer : A go module that send email notifications to users. This module will support HTML and rich text emails. de-stats : Service for obtaining CyVerse Discovery Environment stats and metrics. de-webhooks : A service that listens to AMQP queues for DE notifications, check if the user has webhooks defined for that notification type and then post the notification to webhook if one is defined. dewey : An AMQP message based iRODS indexer for elasticsearch. discoenv-analyses : A small service that makes analysis-related information available over NATS. discoenv-users : Is a microservice for the CyVerse Discovery Environment which allows users to look up information about a user. It uses NATS for communication. email-requests : A simple service to wait for email requests to arrive over AMQP and forward them to cyverse-email. event-recorder : This service listens to an AMQP topic for events, and records events that may be of interest to users in the notifications database. get-analysis-id : Microservice that uses the DE's apps service to return the analysis UUID associated with the provided external UUID. grouper : - grouper-loader & grouper-ws TODO: FIND the repo. - From kubectl apply group-propagator : info-typer : An AMQP message based info type detector infosquito2 : TODO FIND description. iplant-groups : A RESTful facade in front of Grouper . jex-adapter : TODO FIND description. job-status-recorder : TODO FIND description. job-status-listener : Listens over HTTP for job status updates, then publishes them to AMQP. job-status-to-apps-adapter : kifshare : A simple web page that allows users to download publicly available files without a CyVerse account. local-exim (exim-sender) : TODO: FIND the repo. - From kubectl metadata : The REST API for the Discovery Environment Metadata services. monkey : This is a service that synchronizes the tag documents in the data search index with the metadata database monitoring-agent : A service for the Discovery Environment that checks on the state of various internal and external resources and reports that state back to a central data aggregation service for later export to Prometheus. notifications : This service provides the RESTful API for the revised notification system. permissions : This service manages permissions for the Discovery environment. requests : Service for managing administrative requests in the CyVerse Discovery Environment. terrain : Terrain provides the primary REST API used by the Discovery Environment. Its role is to validate user authentication and to coordinate calls to other web services. For more information, please see the Discovery Environment API Documentation . resource-usage-api : is a microservice developed as part of the CyVerse Discovery Environment that provides access to resource usage values (CPU hours, memory, etc.) consumed by users over a customizable time period. saved-searches : A service for the CyVerse Discovery Environment that provides CRUD access to a user's saved searches. search : This is a service which serves as a search facade for the DE and others to use. It uses the querydsl library under the covers to translate requests and provide documentation, then passes off queries to configured elasticsearch servers. sonora : UI for the Discovery Environment templeton : includes templeton-incremental & templeton-periodic TODO: add description. timelord : timelord periodically queries the DE database for running applications and kills any of them that have gone over their time limit. unleash : TODO: FIND the repo. - From kubectl apply user-info : A service for getting user-related information like sessions and preferences. vice-default-backend : Provides a default backend handler for the Kubernetes Ingress that handles routing for VICE apps. This backend decides whether to redirect requests to the loading page service, the landing page service, or to a 404 page depending on whether the URL is valid or not. qms-adapter : Forwards usage information gathered within the Discovery Environment to the Quota Management System QMS . qms : QMS is the CyVerse Quota Management System. Its purpose is to keep track of resource usage limits and totals for CyVerse users. irods-csi-driver : iRODS Container Storage Interface (CSI) Driver implements the CSI Specification to provide container orchestration engines (like Kubernetes) iRODS access. dataone-indexer : Event indexer for the DataONE member node service.","title":"List of core-service"},{"location":"services/kubernetes/#list-of-non-core-services","text":"redis-ha redis-haproxy elasticsearch keycloak openebs","title":"List of non-core services"},{"location":"services/kubernetes/ns/","text":"Namespaces \u00b6 List of namespaces used for CyVerse deployment in kubernetes. Note : this documentation is relavant only for single/current environment. prod \u00b6 This namespace is dedicated to the core services & some of non-core services such as : redis-ha redis-haproxy elasticsearch Except the irods-csi-driver which runs in a specific namespace. ingress-nginx \u00b6 This namespace is dedicated to the ingress-nginx deployment. vice-apps \u00b6 This namespace is dedicated to the VICE related deployment and configurations. keycloak \u00b6 This namespace is dedicated to Keycloak related deployment and configurations. openebs \u00b6 This namespace is dedicated to the openebs deployment. OpenEBS turns any storage available on the Kubernetes worker nodes into local or distributed Kubernetes Persistent Volumes. irods-csi-driver \u00b6 This namespace is dedicated to the irods-csi-driver deployment and configurations.","title":"Namespaces"},{"location":"services/kubernetes/ns/#namespaces","text":"List of namespaces used for CyVerse deployment in kubernetes. Note : this documentation is relavant only for single/current environment.","title":"Namespaces"},{"location":"services/kubernetes/ns/#prod","text":"This namespace is dedicated to the core services & some of non-core services such as : redis-ha redis-haproxy elasticsearch Except the irods-csi-driver which runs in a specific namespace.","title":"prod"},{"location":"services/kubernetes/ns/#ingress-nginx","text":"This namespace is dedicated to the ingress-nginx deployment.","title":"ingress-nginx"},{"location":"services/kubernetes/ns/#vice-apps","text":"This namespace is dedicated to the VICE related deployment and configurations.","title":"vice-apps"},{"location":"services/kubernetes/ns/#keycloak","text":"This namespace is dedicated to Keycloak related deployment and configurations.","title":"keycloak"},{"location":"services/kubernetes/ns/#openebs","text":"This namespace is dedicated to the openebs deployment. OpenEBS turns any storage available on the Kubernetes worker nodes into local or distributed Kubernetes Persistent Volumes.","title":"openebs"},{"location":"services/kubernetes/ns/#irods-csi-driver","text":"This namespace is dedicated to the irods-csi-driver deployment and configurations.","title":"irods-csi-driver"}]}